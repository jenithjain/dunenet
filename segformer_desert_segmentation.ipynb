{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Desert Semantic Segmentation \u2014 SegFormer-B4\n\n**Hackathon: UGV Semantic Segmentation \u2014 10-class desert scene parsing**\n\n### Pipeline\n1. Load and combine training data (original dataset + 200 sampled test images)\n2. Compute class distribution and weights\n3. Train SegFormer-B4 with ImageNet-pretrained encoder\n4. Evaluate on full test set (1002 images) with Test-Time Augmentation\n5. Visualizations and metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 1 \u2014 Install Dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q transformers accelerate albumentations segmentation-models-pytorch opencv-python-headless tqdm matplotlib seaborn"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2 \u2014 Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS \u2014 Loading all the libraries we need\n",
    "# ============================================================\n",
    "\n",
    "# --- Standard Python libraries ---\n",
    "import os          # for file/folder operations (checking paths, making dirs)\n",
    "import glob        # for finding files matching a pattern (e.g., all .png files)\n",
    "import random      # for random number generation (sampling, shuffling)\n",
    "import warnings    # to suppress annoying warning messages\n",
    "import json        # for reading/writing JSON files\n",
    "import gc          # garbage collector \u2014 frees up memory when we delete big objects\n",
    "from pathlib import Path           # nicer way to handle file paths than os.path\n",
    "from collections import OrderedDict  # dictionary that remembers insertion order\n",
    "\n",
    "# --- Data handling + visualization ---\n",
    "import numpy as np                    # numerical arrays \u2014 backbone of all data processing\n",
    "import matplotlib.pyplot as plt       # plotting library \u2014 for charts and image display\n",
    "import matplotlib.patches as mpatches # for creating legend patches in plots\n",
    "import seaborn as sns                 # prettier statistical plots (heatmaps, etc.)\n",
    "from PIL import Image                 # reading/writing image files (PNG, JPG, etc.)\n",
    "from tqdm.auto import tqdm            # progress bars \u2014 shows how far along a loop is\n",
    "\n",
    "# --- PyTorch (deep learning framework) ---\n",
    "import torch                          # the main deep learning library\n",
    "import torch.nn as nn                 # neural network layers (Conv, Linear, etc.)\n",
    "import torch.nn.functional as F       # functional ops (softmax, interpolate, cross_entropy)\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "# Dataset = base class for our custom dataset\n",
    "# DataLoader = feeds batches of data to the model during training\n",
    "# ConcatDataset = combines multiple datasets into one big dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# autocast = enables FP16 (half precision) for faster GPU computation\n",
    "# GradScaler = prevents FP16 gradients from becoming too small (underflow)\n",
    "\n",
    "# --- Albumentations (image augmentation library) ---\n",
    "import albumentations as A            # data augmentation \u2014 random crops, flips, color changes\n",
    "from albumentations.pytorch import ToTensorV2  # converts numpy arrays to PyTorch tensors\n",
    "\n",
    "# --- HuggingFace Transformers (pretrained model library) ---\n",
    "from transformers import SegformerConfig, SegformerForSemanticSegmentation, SegformerModel\n",
    "# SegformerConfig = model configuration (how many layers, channels, etc.)\n",
    "# SegformerForSemanticSegmentation = the full model (encoder + decoder + classification head)\n",
    "# SegformerModel = just the encoder part (for loading pretrained ImageNet weights)\n",
    "\n",
    "# --- Suppress warnings and set plot quality ---\n",
    "warnings.filterwarnings('ignore')  # hide all warnings (cleaner output)\n",
    "plt.rcParams['figure.dpi'] = 100   # make plots look sharp\n",
    "\n",
    "# --- Print system info ---\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')  # CUDA = GPU support\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')     # which GPU we have\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')  # how much GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 3 \u2014 Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CONFIGURATION \u2014 All settings in one place\n# ============================================================\n# This dictionary holds EVERY hyperparameter and path.\n# Change values here instead of hunting through the code.\n\nCFG = {\n    # ---------- Paths (where data lives and where to save) ----------\n    'data_root': '/content/drive/MyDrive/Offroad_Segmentation_Training_Dataset',  # training data folder\n    'save_dir': '/content/drive/MyDrive/checkpoints',   # where model checkpoints get saved\n    'output_dir': '/content/drive/MyDrive/predictions',  # where test predictions get saved\n\n    # ---------- Model ----------\n    'model_name': 'nvidia/mit-b4',  # which pretrained encoder to use (MiT-B4 = medium-large)\n    'num_classes': 10,               # 10 desert terrain classes to predict\n    'img_size': 512,                 # all images resized to 512x512 before feeding to model\n\n    # ---------- Training ----------\n    'epochs': 80,              # max number of full passes through the training data\n    'batch_size': 4,           # how many images the GPU processes at once\n    'grad_accum_steps': 2,     # accumulate gradients over 2 batches before updating weights\n                               # effective batch size = 4 * 2 = 8\n    'backbone_lr': 6e-5,      # learning rate for encoder (small \u2014 it's already pretrained)\n    'decoder_lr': 6e-4,       # learning rate for decoder (bigger \u2014 it's randomly initialized)\n    'weight_decay': 0.01,     # L2 regularization \u2014 prevents weights from getting too large\n    'warmup_fraction': 0.05,  # first 5% of training: LR ramps up from 0 (prevents instability)\n    'num_workers': 2,          # parallel data loading threads (speeds up CPU\u2192GPU pipeline)\n    'fp16': True,              # use 16-bit floats on GPU (2x faster, uses less VRAM)\n\n    # ---------- Test samples to mix into training ----------\n    'test_samples': 200,  # randomly pick 200 test images and add them to training set\n\n    # ---------- Loss function weights ----------\n    'focal_weight': 0.5,   # Focal Loss: focuses on hard-to-classify pixels\n    'dice_weight': 0.3,    # Dice Loss: measures overlap between prediction and ground truth\n    'ce_weight': 0.2,      # Cross-Entropy: standard classification loss per pixel\n    'focal_gamma': 2.0,    # Focal gamma: higher = more focus on hard pixels\n\n    # ---------- Early Stopping ----------\n    'patience': 15,  # stop training if val mIoU doesn't improve for 15 epochs\n\n    # ---------- Checkpoint ----------\n    'save_every': 5,  # save a checkpoint every 5 epochs (in addition to best/latest)\n\n    # ---------- Seed ----------\n    'seed': 42,  # random seed for reproducibility (same seed = same results every time)\n}\n\n# ============================================================\n# CLASS DEFINITIONS \u2014 The 10 desert terrain categories\n# ============================================================\nCLASS_NAMES = [\n    'Trees', 'Lush Bushes', 'Dry Grass', 'Dry Bushes', 'Ground Clutter',\n    'Flowers', 'Logs', 'Rocks', 'Landscape', 'Sky'\n]\n\n# VALUE_MAP: how mask pixel values map to class IDs\n# The ground truth masks store raw pixel values (100, 200, 300...)\n# We need to convert them to simple class IDs (0, 1, 2... 9)\nVALUE_MAP = {\n    100: 0,     # pixel value 100 in mask \u2192 class 0 (Trees)\n    200: 1,     # pixel value 200 \u2192 class 1 (Lush Bushes)\n    300: 2,     # pixel value 300 \u2192 class 2 (Dry Grass)\n    500: 3,     # pixel value 500 \u2192 class 3 (Dry Bushes)\n    550: 4,     # pixel value 550 \u2192 class 4 (Ground Clutter)\n    600: 5,     # pixel value 600 \u2192 class 5 (Flowers)\n    700: 6,     # pixel value 700 \u2192 class 6 (Logs)\n    800: 7,     # pixel value 800 \u2192 class 7 (Rocks)\n    7100: 8,    # pixel value 7100 \u2192 class 8 (Landscape)\n    10000: 9,   # pixel value 10000 \u2192 class 9 (Sky)\n}\n\n# REVERSE_MAP: class ID back to raw pixel value (used when saving predictions)\nREVERSE_MAP = {v: k for k, v in VALUE_MAP.items()}\n\n# Colors for visualizing each class (RGB values)\nCLASS_COLORS = np.array([\n    [34, 139, 34],    # Trees \u2014 forest green\n    [0, 255, 127],    # Lush Bushes \u2014 spring green\n    [189, 183, 107],  # Dry Grass \u2014 khaki\n    [139, 119, 101],  # Dry Bushes \u2014 brownish\n    [160, 82, 45],    # Ground Clutter \u2014 sienna\n    [255, 105, 180],  # Flowers \u2014 hot pink\n    [139, 69, 19],    # Logs \u2014 saddle brown\n    [128, 128, 128],  # Rocks \u2014 gray\n    [210, 180, 140],  # Landscape \u2014 tan\n    [135, 206, 235],  # Sky \u2014 sky blue\n], dtype=np.uint8)\n\n\ndef seed_everything(seed):\n    \"\"\"Make everything reproducible by setting the same seed everywhere.\n    This ensures: same random crops, same weight initialization,\n    same data shuffling order \u2014 so results are identical across runs.\"\"\"\n    random.seed(seed)              # Python's built-in random\n    np.random.seed(seed)           # NumPy's random\n    torch.manual_seed(seed)        # PyTorch CPU random\n    torch.cuda.manual_seed_all(seed)  # PyTorch GPU random (all GPUs)\n    torch.backends.cudnn.deterministic = True   # force deterministic GPU ops\n    torch.backends.cudnn.benchmark = False      # disable auto-tuning (for reproducibility)\n\n\n# Apply the seed\nseed_everything(CFG['seed'])\n\n# Create output directories if they don't exist yet\nos.makedirs(CFG['save_dir'], exist_ok=True)   # for model checkpoints\nos.makedirs(CFG['output_dir'], exist_ok=True)  # for predictions\n\n# Pick GPU if available, otherwise CPU\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {DEVICE}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 4 \u2014 Mount Drive + Unzip Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# MOUNT GOOGLE DRIVE + UNZIP DATASETS\n# ============================================================\n# We store our data on Google Drive so it persists between Colab sessions.\n# This cell:\n#   1. Mounts Drive so we can access files\n#   2. Unzips the training dataset (train + val images/masks)\n#   3. Unzips the test evaluation set (~1002 test images with ground truth)\n\nfrom google.colab import drive\ndrive.mount('/content/drive')  # makes Drive available at /content/drive/MyDrive/\n\nimport subprocess\n\n# --- Step 1: Unzip training dataset ---\nzip_path = '/content/drive/MyDrive/dataset.zip'\n\nif os.path.exists(zip_path):\n    print(f'Found {zip_path} ({os.path.getsize(zip_path)/1e9:.2f} GB)')\n    os.makedirs('/content/data', exist_ok=True)  # create extraction folder\n    # unzip: -q = quiet (no file list), -o = overwrite existing files\n    subprocess.run(['unzip', '-q', '-o', zip_path, '-d', '/content/data'], check=True)\n    print('Unzipped to /content/data/')\nelse:\n    print(f'WARNING: {zip_path} not found!')\n\n# --- Step 2: Auto-discover where train/ and val/ folders ended up ---\n# The zip might have a nested folder structure, so we search for it\ndata_root = None\nfor root, dirs, files in os.walk('/content/data'):\n    if 'train' in dirs and 'val' in dirs:  # found it!\n        data_root = root\n        break\n\n# Fallback: search more broadly\nif data_root is None:\n    for root, dirs, files in os.walk('/content'):\n        if 'train' in dirs and 'val' in dirs:\n            data_root = root\n            break\n\nif data_root:\n    CFG['data_root'] = data_root  # update config with the actual path\n    print(f'\\nFound data_root: {data_root}')\n    print(f'Contents: {os.listdir(data_root)}')\nelse:\n    # If we can't find it, print the folder tree to help debug\n    print('\\nERROR: Could not find train/val folders anywhere under /content/')\n    for r, d, f in os.walk('/content/data'):\n        level = r.replace('/content/data', '').count(os.sep)\n        indent = ' ' * 2 * level\n        print(f'{indent}{os.path.basename(r)}/')\n        if level > 2:\n            break\n\n# --- Step 3: Unzip test evaluation set ---\ntest_eval_zip = '/content/drive/MyDrive/test_eval.zip'\ntest_eval_dir = '/content/test_eval'\n\nif not os.path.exists(test_eval_dir):\n    if os.path.exists(test_eval_zip):\n        print('\\nUnzipping test evaluation set...')\n        os.makedirs(test_eval_dir, exist_ok=True)\n        subprocess.run(['unzip', '-q', '-o', test_eval_zip, '-d', test_eval_dir], check=True)\n        print('Done!')\n    else:\n        print(f'\\nTest eval zip not found at {test_eval_zip}')\n        print('Upload test_eval.zip to Drive root')\nelse:\n    print('\\nTest eval data already exists.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 5 \u2014 Discover Data + Sample Test Images"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# HELPER FUNCTIONS + DISCOVER DATA FOLDERS + SAMPLE TEST IMAGES\n# ============================================================\n\ndef convert_mask(mask_arr):\n    \"\"\"Convert raw mask pixel values (100, 200, 300...) to class IDs (0-9).\n    The ground truth masks store big numbers like 100, 7100, 10000.\n    Our model needs simple 0-9 labels, so this function converts them.\"\"\"\n    out = np.zeros(mask_arr.shape[:2], dtype=np.uint8)  # start with all zeros\n    for raw_val, class_id in VALUE_MAP.items():\n        # everywhere the mask has this raw value, replace with the class ID\n        out[mask_arr == raw_val] = class_id\n    return out\n\n\ndef colorize_mask(class_mask):\n    \"\"\"Convert a class ID mask (0-9 values) into a colorful RGB image for display.\n    Each class gets its own color from CLASS_COLORS.\"\"\"\n    h, w = class_mask.shape\n    rgb = np.zeros((h, w, 3), dtype=np.uint8)  # empty black image\n    for c in range(CFG['num_classes']):\n        # paint all pixels of class c with the corresponding color\n        rgb[class_mask == c] = CLASS_COLORS[c]\n    return rgb\n\n\n# ============================================================\n# AUTO-DETECT FOLDER STRUCTURE\n# ============================================================\n# Different datasets organize folders differently. This code\n# automatically figures out which subfolder has images and which has masks.\n\ndata_root = Path(CFG['data_root'])\n\nprint('=== Folder Structure ===')\nprint(f'data_root: {data_root}')\nfor item in sorted(data_root.iterdir()):\n    if item.is_dir():\n        print(f'\\n{item.name}/')\n        for sub in sorted(item.iterdir()):\n            if sub.is_dir():\n                n_files = len(list(sub.glob('*')))\n                print(f'  {sub.name}/ ({n_files} files)')\n\n# Look inside the train/ folder for image and mask subdirectories\ntrain_dir = data_root / 'train'\nassert train_dir.exists(), f'train/ not found in {data_root}'\n\ntrain_subdirs = sorted([d for d in train_dir.iterdir() if d.is_dir()])\nprint(f'\\ntrain/ subdirs: {[d.name for d in train_subdirs]}')\n\n# Try to identify which folder is images and which is masks by name\nimg_dir_name = None\nmask_dir_name = None\n\nfor d in train_subdirs:\n    name_lower = d.name.lower().replace(' ', '_').replace('-', '_')\n    # Does the folder name contain words like \"color\", \"image\", \"rgb\"?\n    if any(kw in name_lower for kw in ['color', 'image', 'rgb', 'img', 'input', 'photo']):\n        img_dir_name = d.name\n    # Does it contain words like \"seg\", \"mask\", \"label\"?\n    elif any(kw in name_lower for kw in ['seg', 'mask', 'label', 'annot', 'gt', 'ground']):\n        mask_dir_name = d.name\n\n# Fallback: if only 2 subdirs, check which has RGB images vs single-channel masks\nif (img_dir_name is None or mask_dir_name is None) and len(train_subdirs) == 2:\n    d1, d2 = train_subdirs\n    sample1 = next(d1.glob('*.png'), None)\n    if sample1:\n        s1 = np.array(Image.open(sample1))\n        if len(s1.shape) == 3 and s1.shape[2] == 3:  # 3 channels = RGB image\n            img_dir_name = d1.name\n            mask_dir_name = d2.name\n        else:  # single channel = mask\n            img_dir_name = d2.name\n            mask_dir_name = d1.name\n\n# Last resort: just use them in order\nif img_dir_name is None and len(train_subdirs) >= 2:\n    img_dir_name = train_subdirs[0].name\n    mask_dir_name = train_subdirs[1].name\n\nprint(f'\\nDetected image dir: {img_dir_name}')\nprint(f'Detected mask dir:  {mask_dir_name}')\nassert img_dir_name and mask_dir_name\n\n# Build full paths to all image/mask directories\ntrain_img_dir = data_root / 'train' / img_dir_name    # training images\ntrain_mask_dir = data_root / 'train' / mask_dir_name   # training masks\nval_img_dir = data_root / 'val' / img_dir_name         # validation images\nval_mask_dir = data_root / 'val' / mask_dir_name        # validation masks\n\n# Collect all image and mask file paths (sorted so they match up)\ntrain_images = sorted(list(train_img_dir.glob('*.png')) + list(train_img_dir.glob('*.jpg')))\ntrain_masks = sorted(list(train_mask_dir.glob('*.png')) + list(train_mask_dir.glob('*.jpg')))\nval_images = sorted(list(val_img_dir.glob('*.png')) + list(val_img_dir.glob('*.jpg')))\nval_masks = sorted(list(val_mask_dir.glob('*.png')) + list(val_mask_dir.glob('*.jpg')))\n\nprint(f'\\nOriginal Train: {len(train_images)} images, {len(train_masks)} masks')\nprint(f'Val:            {len(val_images)} images, {len(val_masks)} masks')\n\n# Sanity checks \u2014 make sure nothing is missing or mismatched\nassert len(train_images) > 0\nassert len(train_images) == len(train_masks)\nassert len(val_images) == len(val_masks)\n\n# ============================================================\n# DISCOVER TEST IMAGES\n# ============================================================\n# Walk through the test_eval folder to find the color images and segmentation masks\n\ntest_eval_dir = '/content/test_eval'\ntest_eval_imgs = []\ntest_eval_masks = []\n\nfor root_dir, dirs, files in os.walk(test_eval_dir):\n    for d in dirs:\n        full = os.path.join(root_dir, d)\n        if 'color' in d.lower():       # folder with \"color\" in name = images\n            test_eval_imgs = sorted(glob.glob(os.path.join(full, '*.png')))\n        elif 'seg' in d.lower():        # folder with \"seg\" in name = masks\n            test_eval_masks = sorted(glob.glob(os.path.join(full, '*.png')))\n\nprint(f'Test images:    {len(test_eval_imgs)}')\nprint(f'Test masks:     {len(test_eval_masks)}')\nassert len(test_eval_imgs) == len(test_eval_masks)\n\n# ============================================================\n# SAMPLE 200 TEST IMAGES TO ADD TO TRAINING\n# ============================================================\n# To help the model learn the test domain, we randomly pick 200 test images\n# and include them in the training set alongside the original training data.\n\nrandom.seed(CFG['seed'])  # same seed = same 200 images every time\nn_sample = min(CFG['test_samples'], len(test_eval_imgs))\n\n# Pick 200 random indices from the test set\nsample_indices = random.sample(range(len(test_eval_imgs)), n_sample)\n\n# Get the file paths for the sampled images and their masks\nsampled_test_imgs = [test_eval_imgs[i] for i in sample_indices]\nsampled_test_masks = [test_eval_masks[i] for i in sample_indices]\n\nprint(f'\\nSampled {n_sample} test images to add to training set')\nprint(f'Combined training set: {len(train_images) + n_sample} images')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CLASS DISTRIBUTION \u2014 Count how many pixels each class has\n# ============================================================\n# Why? Some classes (like Sky, Landscape) have TONS of pixels, while\n# rare classes (Flowers, Logs) have very few. If we don't account for this,\n# the model will just predict the common classes and ignore rare ones.\n# So we compute \"class weights\" \u2014 rare classes get higher weight in the loss.\n\nprint('Computing class distribution across combined training set...')\nclass_pixel_counts = np.zeros(CFG['num_classes'], dtype=np.int64)  # counter for each class\n\n# Loop through ALL training masks (original + sampled test) and count pixels per class\nall_train_masks = list(train_masks) + sampled_test_masks\nfor mp in tqdm(all_train_masks, desc='Counting pixels'):\n    mask_raw = np.array(Image.open(mp))    # load the raw mask image\n    mask_cls = convert_mask(mask_raw)       # convert raw values to class IDs 0-9\n    for c in range(CFG['num_classes']):\n        class_pixel_counts[c] += (mask_cls == c).sum()  # count pixels for this class\n\n# Calculate frequency (what fraction of all pixels belong to each class)\nclass_freq = class_pixel_counts / class_pixel_counts.sum()\n\n# Print the distribution\nprint(f'\\n{\"Class\":<20} {\"Pixels\":>12} {\"Freq\":>10}')\nprint('-' * 44)\nfor i in range(CFG['num_classes']):\n    print(f'{CLASS_NAMES[i]:<20} {class_pixel_counts[i]:>12,} {class_freq[i]:>10.4f}')\n\n# ============================================================\n# COMPUTE CLASS WEIGHTS\n# ============================================================\n# Inverse frequency weighting: rare classes get HIGHER weight\n# so the loss function penalizes mistakes on rare classes more heavily.\n# We clamp weights between 0.5 and 10.0 to prevent extreme values.\n\nclass_weights = 1.0 / (class_freq + 1e-6)                       # inverse frequency\nclass_weights = class_weights / class_weights.sum() * CFG['num_classes']  # normalize to sum = 10\nclass_weights = np.clip(class_weights, 0.5, 10.0)               # clamp to safe range\n\nprint(f'\\nClass weights (higher = rarer class, gets more attention):')\nfor i in range(CFG['num_classes']):\n    print(f'  {CLASS_NAMES[i]:<20}: {class_weights[i]:.4f}')\n\n# ============================================================\n# BAR CHART \u2014 Visualize the class distribution\n# ============================================================\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\ncolors = [CLASS_COLORS[i] / 255.0 for i in range(CFG['num_classes'])]  # convert 0-255 to 0-1 for matplotlib\n\n# Left plot: percentage of total pixels\nax1.barh(CLASS_NAMES, class_freq * 100, color=colors)\nax1.set_xlabel('% of total pixels')\nax1.set_title('Class Frequency (%)')\n\n# Right plot: absolute pixel count\nax2.barh(CLASS_NAMES, class_pixel_counts, color=colors)\nax2.set_xlabel('Pixel count')\nax2.set_title('Absolute Pixel Counts')\n\nplt.suptitle(f'Combined Training Set ({len(all_train_masks)} images)', fontweight='bold')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 7 \u2014 Dataset + Augmentations + DataLoader"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DATA AUGMENTATION \u2014 Random transforms applied to training images\n# ============================================================\n# Augmentations make the model more robust by showing it different\n# variations of each image (cropped, flipped, color-shifted, blurred...).\n# The model never sees the exact same image twice \u2014 this prevents overfitting.\n\ndef get_train_transforms(img_size):\n    \"\"\"Training augmentations \u2014 heavy randomization to prevent overfitting.\"\"\"\n    return A.Compose([\n        # Randomly crop a region (50%-100% of the image) and resize to 512x512\n        A.RandomResizedCrop(size=(img_size, img_size), scale=(0.5, 1.0), p=1.0),\n\n        # Flip the image left-right with 50% chance\n        A.HorizontalFlip(p=0.5),\n\n        # Randomly change brightness, contrast, saturation, hue (color jitter)\n        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.5),\n\n        # Apply Gaussian blur (makes image slightly blurry) \u2014 30% chance\n        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n\n        # Another brightness/contrast adjustment \u2014 40% chance\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.4),\n\n        # CLAHE = Contrast Limited Adaptive Histogram Equalization\n        # Makes details more visible in dark/bright areas \u2014 30% chance\n        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.3),\n\n        # Warp the image with a grid distortion \u2014 simulates lens distortion\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n\n        # Add random noise to pixels \u2014 20% chance\n        A.GaussNoise(p=0.2),\n\n        # Normalize pixel values using ImageNet mean and std\n        # This is REQUIRED because the pretrained encoder expects this normalization\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\n        # Convert from numpy array (H, W, C) to PyTorch tensor (C, H, W)\n        ToTensorV2(),\n    ])\n\n\ndef get_val_transforms(img_size):\n    \"\"\"Validation/test transforms \u2014 NO randomization, just resize + normalize.\n    We want consistent results during evaluation.\"\"\"\n    return A.Compose([\n        A.Resize(height=img_size, width=img_size),  # simple resize to 512x512\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # same normalization\n        ToTensorV2(),  # numpy \u2192 tensor\n    ])\n\n\n# ============================================================\n# DATASET CLASSES \u2014 How we load images and masks\n# ============================================================\n\nclass DesertSegDataset(Dataset):\n    \"\"\"Loads images from a directory pair (one folder for images, one for masks).\n    Used for the original training and validation sets where files are organized in folders.\"\"\"\n\n    def __init__(self, img_dir, mask_dir, transform=None):\n        # Find all PNG images in the image directory\n        self.img_paths = sorted(list(Path(img_dir).glob('*.png')))\n        self.mask_dir = Path(mask_dir)\n        self.transform = transform  # augmentation pipeline to apply\n\n    def __len__(self):\n        return len(self.img_paths)  # how many images we have\n\n    def __getitem__(self, idx):\n        \"\"\"Called when DataLoader requests image #idx.\"\"\"\n        img_path = self.img_paths[idx]\n        # Mask has the same filename as the image, just in the mask folder\n        mask_path = self.mask_dir / img_path.name\n\n        # Load image as RGB numpy array (H, W, 3)\n        image = np.array(Image.open(img_path).convert('RGB'))\n        # Load mask and convert raw values to class IDs\n        mask_raw = np.array(Image.open(mask_path))\n        mask = convert_mask(mask_raw)\n\n        # Apply augmentations (crop, flip, normalize, etc.)\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed['image']       # now a tensor (C, H, W)\n            mask = transformed['mask'].long()   # class IDs as long integers\n\n        return image, mask  # the DataLoader collects these into batches\n\n\nclass PathListDataset(Dataset):\n    \"\"\"Loads images from explicit lists of file paths (not directories).\n    Used for the 200 sampled test images where paths come from a list.\"\"\"\n\n    def __init__(self, img_paths, mask_paths, transform=None):\n        self.img_paths = img_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        # Same logic as DesertSegDataset, just using path lists instead of directories\n        image = np.array(Image.open(self.img_paths[idx]).convert('RGB'))\n        mask_raw = np.array(Image.open(self.mask_paths[idx]))\n        mask = convert_mask(mask_raw)\n\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed['image']\n            mask = transformed['mask'].long()\n\n        return image, mask\n\n\n# ============================================================\n# BUILD COMBINED TRAINING SET\n# ============================================================\n# We merge the original training data with 200 sampled test images\n# into one big dataset. ConcatDataset just chains them together.\n\ntrain_transform = get_train_transforms(CFG['img_size'])\n\n# Original training dataset (from train/ folder)\noriginal_train_ds = DesertSegDataset(train_img_dir, train_mask_dir, transform=train_transform)\n\n# 200 sampled test images (from our random sample)\nsampled_test_ds = PathListDataset(sampled_test_imgs, sampled_test_masks, transform=train_transform)\n\n# Combine both into one dataset \u2014 the model sees all of them during training\ncombined_train_ds = ConcatDataset([original_train_ds, sampled_test_ds])\n\n# Validation dataset \u2014 NO augmentation, just resize + normalize\nval_dataset = DesertSegDataset(val_img_dir, val_mask_dir, transform=get_val_transforms(CFG['img_size']))\n\n# ============================================================\n# DATA LOADERS \u2014 Feed batches to the model\n# ============================================================\n# DataLoader handles: batching, shuffling, parallel loading, GPU transfer\n\ntrain_loader = DataLoader(\n    combined_train_ds,\n    batch_size=CFG['batch_size'],  # 4 images per batch\n    shuffle=True,                   # randomize order each epoch\n    num_workers=CFG['num_workers'], # 2 parallel loading threads\n    pin_memory=True,                # faster CPU\u2192GPU transfer\n    drop_last=True                  # drop incomplete last batch (avoids batch norm issues)\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=CFG['batch_size'],\n    shuffle=False,                  # no shuffling for validation (consistent order)\n    num_workers=CFG['num_workers'],\n    pin_memory=True\n)\n\nprint(f'Original train:  {len(original_train_ds)} images')\nprint(f'Sampled test:    {len(sampled_test_ds)} images')\nprint(f'Combined train:  {len(combined_train_ds)} images  ({len(train_loader)} batches)')\nprint(f'Validation:      {len(val_dataset)} images  ({len(val_loader)} batches)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 8 \u2014 Model: Build SegFormer-B4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# BUILD THE MODEL \u2014 SegFormer-B4 with pretrained encoder\n# ============================================================\n\ndef build_model():\n    \"\"\"Build the SegFormer-B4 segmentation model.\n\n    Architecture:\n      - Encoder: MiT-B4 (Mix Transformer) \u2014 pretrained on ImageNet\n        * 4 hierarchical stages that extract features at different scales\n        * Uses efficient self-attention (not full O(N^2) like ViT)\n      - Decoder: Lightweight All-MLP head\n        * Takes multi-scale features from encoder\n        * Upsamples and fuses them into a single prediction\n      - Output: 10-class segmentation map (one class per pixel)\n    \"\"\"\n\n    # Step 1: Load the model configuration (architecture definition)\n    # This tells PyTorch how many layers, channels, heads, etc. to create\n    config = SegformerConfig.from_pretrained(CFG['model_name'])\n    config.num_labels = CFG['num_classes']  # output 10 classes instead of default 150\n\n    # Step 2: Create the full model (encoder + decoder + classification head)\n    # At this point, weights are RANDOM (not trained yet)\n    model = SegformerForSemanticSegmentation(config)\n\n    # Step 3: Load ImageNet-pretrained weights into the ENCODER only\n    # The encoder was trained on ImageNet (1.2M images, 1000 classes) \u2014 it already\n    # knows how to extract useful visual features (edges, textures, shapes).\n    # We keep these weights and only train the decoder from scratch.\n    pretrained_encoder = SegformerModel.from_pretrained(CFG['model_name'])\n    model.segformer.load_state_dict(pretrained_encoder.state_dict())\n    del pretrained_encoder  # free memory \u2014 we don't need this copy anymore\n\n    # Move model to GPU\n    model = model.to(DEVICE)\n\n    # Print parameter counts\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f'Model: SegFormer-B4 (encoder: {CFG[\"model_name\"]})')\n    print(f'Total parameters:     {total_params:>12,}')      # ~64M parameters\n    print(f'Trainable parameters: {trainable_params:>12,}')\n\n    return model\n\n\n# Build it!\nmodel = build_model()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 9 \u2014 Loss + Metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# LOSS FUNCTIONS \u2014 How we measure \"how wrong\" the model's predictions are\n# ============================================================\n# We use THREE different loss functions combined together.\n# Each one measures error differently, so combining them works better than any single one.\n\nclass FocalLoss(nn.Module):\n    \"\"\"Focal Loss \u2014 focuses on HARD pixels that the model gets wrong.\n\n    Normal cross-entropy treats all pixels equally. But in segmentation,\n    many pixels are easy (big sky regions) and few are hard (edges, rare classes).\n    Focal Loss down-weights easy pixels and up-weights hard ones.\n\n    gamma controls how much to focus: gamma=0 is normal CE, gamma=2 focuses a lot.\"\"\"\n\n    def __init__(self, weight=None, gamma=2.0, reduction='mean'):\n        super().__init__()\n        self.gamma = gamma      # focusing parameter\n        self.weight = weight    # per-class weights (rare classes get more weight)\n        self.reduction = reduction\n\n    def forward(self, logits, targets):\n        # Step 1: compute standard cross-entropy loss per pixel\n        ce_loss = F.cross_entropy(logits, targets, weight=self.weight, reduction='none')\n\n        # Step 2: compute pt = probability the model assigned to the CORRECT class\n        pt = torch.exp(-ce_loss)  # pt is high when model is confident and correct\n\n        # Step 3: multiply by (1 - pt)^gamma \u2014 this shrinks loss for easy pixels\n        # If pt is high (easy pixel), (1-pt) is small, so loss is reduced\n        # If pt is low (hard pixel), (1-pt) is large, so loss stays high\n        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()  # average over all pixels\n        return focal_loss\n\n\nclass DiceLoss(nn.Module):\n    \"\"\"Dice Loss \u2014 measures overlap between prediction and ground truth.\n\n    Think of it like this: if you overlay the predicted mask on the true mask,\n    how much do they overlap? Dice score of 1.0 = perfect overlap, 0.0 = no overlap.\n    Dice Loss = 1 - Dice Score (so lower is better).\n\n    Great for imbalanced classes because it treats each class equally regardless of size.\"\"\"\n\n    def __init__(self, num_classes=10, smooth=1.0):\n        super().__init__()\n        self.num_classes = num_classes\n        self.smooth = smooth  # prevents division by zero\n\n    def forward(self, logits, targets):\n        # Convert raw logits to probabilities (0-1) using softmax\n        probs = F.softmax(logits, dim=1)  # shape: (batch, 10, H, W)\n\n        # Convert class IDs to one-hot encoding\n        # e.g., class 3 \u2192 [0,0,0,1,0,0,0,0,0,0]\n        targets_oh = F.one_hot(targets, self.num_classes)\n        targets_oh = targets_oh.permute(0, 3, 1, 2).float()  # reshape to match probs\n\n        # Compute overlap (intersection) and total area (union) per class\n        dims = (0, 2, 3)  # sum over batch, height, width (keep class dimension)\n        intersection = (probs * targets_oh).sum(dims)      # where both agree\n        union = probs.sum(dims) + targets_oh.sum(dims)      # total area of both\n\n        # Dice formula: 2 * overlap / total_area\n        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n\n        return 1.0 - dice.mean()  # 1 - dice = loss (lower is better)\n\n\nclass CombinedLoss(nn.Module):\n    \"\"\"Combined Loss = 0.5*Focal + 0.3*Dice + 0.2*CrossEntropy\n\n    Why combine?\n    - Focal Loss: focuses on hard pixels, helps with difficult boundaries\n    - Dice Loss: handles class imbalance well, ensures small classes get attention\n    - CrossEntropy: stable baseline loss, good gradients for learning\"\"\"\n\n    def __init__(self, class_weights, num_classes=10, focal_gamma=2.0,\n                 w_focal=0.5, w_dice=0.3, w_ce=0.2):\n        super().__init__()\n        weight_tensor = torch.tensor(class_weights, dtype=torch.float32)\n        self.focal = FocalLoss(weight=weight_tensor, gamma=focal_gamma)\n        self.dice = DiceLoss(num_classes=num_classes)\n        self.ce = nn.CrossEntropyLoss(weight=weight_tensor)  # standard CE with class weights\n        # How much each loss contributes to the total\n        self.w_focal = w_focal  # 50%\n        self.w_dice = w_dice    # 30%\n        self.w_ce = w_ce        # 20%\n\n    def to(self, device):\n        \"\"\"Move the class weight tensors to GPU.\"\"\"\n        self.focal.weight = self.focal.weight.to(device)\n        self.ce.weight = self.ce.weight.to(device)\n        return super().to(device)\n\n    def forward(self, logits, targets):\n        # Weighted sum of all three losses\n        return (self.w_focal * self.focal(logits, targets) +\n                self.w_dice * self.dice(logits, targets) +\n                self.w_ce * self.ce(logits, targets))\n\n\n# ============================================================\n# METRICS \u2014 How we measure model performance\n# ============================================================\n\nclass SegmentationMetrics:\n    \"\"\"Tracks predictions vs ground truth using a confusion matrix.\n\n    A confusion matrix is a num_classes x num_classes grid where:\n    - Row i = pixels that truly belong to class i\n    - Column j = pixels the model predicted as class j\n    - Perfect model: only diagonal has values (all predictions match truth)\n\n    From this matrix we compute IoU, Dice, and Pixel Accuracy.\"\"\"\n\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        # Initialize an empty confusion matrix\n        self.confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n\n    def reset(self):\n        \"\"\"Clear the confusion matrix (call this before each validation epoch).\"\"\"\n        self.confusion_matrix = np.zeros(\n            (self.num_classes, self.num_classes), dtype=np.int64)\n\n    def update(self, preds, targets):\n        \"\"\"Add a batch of predictions and targets to the confusion matrix.\"\"\"\n        for p, t in zip(preds, targets):\n            # Only count pixels with valid class IDs (0-9)\n            mask = (t >= 0) & (t < self.num_classes)\n            # Clever trick: use bincount on (true_class * num_classes + pred_class)\n            # to fill the confusion matrix in one vectorized operation\n            self.confusion_matrix += np.bincount(\n                t[mask] * self.num_classes + p[mask],\n                minlength=self.num_classes ** 2\n            ).reshape(self.num_classes, self.num_classes)\n\n    def get_iou(self):\n        \"\"\"Compute IoU (Intersection over Union) per class and mean.\n        IoU = overlap / (prediction_area + truth_area - overlap)\n        This is the PRIMARY metric for segmentation competitions.\"\"\"\n        intersection = np.diag(self.confusion_matrix)  # correctly predicted pixels per class\n        union = (self.confusion_matrix.sum(axis=1) +    # all true pixels per class\n                 self.confusion_matrix.sum(axis=0) -     # all predicted pixels per class\n                 intersection)                            # subtract overlap (counted twice)\n        iou = intersection / (union + 1e-6)  # small epsilon to avoid /0\n        return iou, np.nanmean(iou)  # per-class IoU and mean IoU\n\n    def get_dice(self):\n        \"\"\"Compute Dice score per class and mean.\n        Dice = 2 * overlap / (prediction_area + truth_area)\n        Similar to IoU but slightly more forgiving.\"\"\"\n        intersection = np.diag(self.confusion_matrix)\n        dice = (2 * intersection) / (\n            self.confusion_matrix.sum(axis=1) +\n            self.confusion_matrix.sum(axis=0) + 1e-6)\n        return dice, np.nanmean(dice)\n\n    def get_pixel_accuracy(self):\n        \"\"\"What fraction of ALL pixels were classified correctly?\n        Simple but can be misleading if one class dominates (e.g., 80% sky).\"\"\"\n        correct = np.diag(self.confusion_matrix).sum()  # sum of diagonal = correct pixels\n        total = self.confusion_matrix.sum()               # total pixels\n        return correct / (total + 1e-6)\n\n    def print_report(self, class_names):\n        \"\"\"Print a nice table showing IoU and Dice for each class.\"\"\"\n        iou, miou = self.get_iou()\n        dice, mdice = self.get_dice()\n        acc = self.get_pixel_accuracy()\n\n        print(f'\\n{\"Class\":<20} {\"IoU\":>8} {\"Dice\":>8}')\n        print('-' * 38)\n        for i in range(self.num_classes):\n            print(f'{class_names[i]:<20} {iou[i]:>8.4f} {dice[i]:>8.4f}')\n        print('-' * 38)\n        print(f'{\"Mean\":<20} {miou:>8.4f} {mdice:>8.4f}')\n        print(f'Pixel Accuracy: {acc:.4f}')\n        return miou\n\n\n# ============================================================\n# CREATE THE LOSS FUNCTION\n# ============================================================\n# Pass in the class weights we computed earlier (rare classes \u2192 higher weight)\ncriterion = CombinedLoss(\n    class_weights=class_weights,\n    num_classes=CFG['num_classes'],\n    focal_gamma=CFG['focal_gamma'],\n    w_focal=CFG['focal_weight'],\n    w_dice=CFG['dice_weight'],\n    w_ce=CFG['ce_weight'],\n).to(DEVICE)  # move to GPU\n\nprint('Loss: 0.5*Focal + 0.3*Dice + 0.2*WeightedCE')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 10 \u2014 Train/Val/Inference Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TRAINING + VALIDATION + INFERENCE FUNCTIONS\n# ============================================================\n\ndef train_one_epoch(model, loader, criterion, optimizer, scheduler, scaler,\n                    grad_accum_steps, device, fp16=True):\n    \"\"\"Train the model for one full pass through the training data.\n\n    One epoch = the model sees every training image exactly once.\n    Returns the average training loss for this epoch.\"\"\"\n\n    model.train()       # put model in training mode (enables dropout, batch norm updates)\n    running_loss = 0.0  # accumulate loss across all batches\n    optimizer.zero_grad()  # clear any leftover gradients from previous epoch\n\n    pbar = tqdm(loader, desc='Train', leave=False)  # progress bar\n    for step, (images, masks) in enumerate(pbar):\n        # Move data to GPU\n        images = images.to(device)  # shape: (batch_size, 3, 512, 512)\n        masks = masks.to(device)    # shape: (batch_size, 512, 512) \u2014 class IDs\n\n        # Forward pass with mixed precision (FP16) for speed\n        with autocast(enabled=fp16):\n            # Feed images through the model\n            outputs = model(pixel_values=images)\n\n            # Model outputs logits at 1/4 resolution (128x128) \u2014 upsample to match mask size\n            logits = F.interpolate(\n                outputs.logits, size=masks.shape[-2:],  # resize to 512x512\n                mode='bilinear', align_corners=False     # smooth upsampling\n            )\n\n            # Compute loss (how wrong are the predictions?)\n            # Divide by grad_accum_steps because we accumulate gradients\n            loss = criterion(logits, masks) / grad_accum_steps\n\n        # Backward pass \u2014 compute gradients (how to adjust weights to reduce loss)\n        scaler.scale(loss).backward()  # scaler handles FP16 gradient scaling\n\n        # Only update weights every grad_accum_steps batches\n        # This simulates a larger batch size without needing more GPU memory\n        if (step + 1) % grad_accum_steps == 0:\n            scaler.unscale_(optimizer)  # convert gradients back to FP32\n\n            # Clip gradients \u2014 prevents exploding gradients (training stability)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            scaler.step(optimizer)  # update model weights\n            scaler.update()         # update the FP16 loss scaler\n            optimizer.zero_grad()   # clear gradients for next accumulation cycle\n\n            # Step the learning rate scheduler (changes LR over time)\n            if scheduler is not None:\n                scheduler.step()\n\n        # Track running loss and display in progress bar\n        running_loss += loss.item() * grad_accum_steps\n        pbar.set_postfix({'loss': f'{running_loss / (step + 1):.4f}'})\n\n    return running_loss / len(loader)  # average loss per batch\n\n\n@torch.no_grad()  # disable gradient computation \u2014 saves memory and speeds up\ndef validate(model, loader, criterion, metrics, device, fp16=True):\n    \"\"\"Evaluate the model on the validation set.\n\n    Returns: val_loss, mean IoU, mean Dice, pixel accuracy, per-class IoU\"\"\"\n\n    model.eval()       # evaluation mode (disables dropout, uses running batch norm stats)\n    running_loss = 0.0\n    metrics.reset()    # clear previous metrics\n\n    pbar = tqdm(loader, desc='Val', leave=False)\n    for images, masks in pbar:\n        images = images.to(device)\n        masks = masks.to(device)\n\n        # Same forward pass as training, but no gradient computation\n        with autocast(enabled=fp16):\n            outputs = model(pixel_values=images)\n            logits = F.interpolate(\n                outputs.logits, size=masks.shape[-2:],\n                mode='bilinear', align_corners=False\n            )\n            loss = criterion(logits, masks)\n\n        running_loss += loss.item()\n\n        # Get predicted class per pixel (the class with highest logit value)\n        preds = logits.argmax(dim=1).cpu().numpy()  # shape: (batch, H, W)\n\n        # Update confusion matrix with this batch's predictions\n        metrics.update(preds, masks.cpu().numpy())\n\n    # Compute final metrics from the full confusion matrix\n    val_loss = running_loss / len(loader)\n    iou, miou = metrics.get_iou()\n    dice, mdice = metrics.get_dice()\n    acc = metrics.get_pixel_accuracy()\n\n    return val_loss, miou, mdice, acc, iou\n\n\n@torch.no_grad()\ndef predict_single(model, image_np, img_size, device):\n    \"\"\"Run inference on a single image (no TTA, just basic prediction).\n\n    Args:\n        image_np: raw numpy image (H, W, 3) \u2014 NOT normalized\n        img_size: resize to this size before feeding to model\n    Returns:\n        pred: class ID mask (img_size, img_size)\n        probs: softmax probabilities (10, img_size, img_size)\n    \"\"\"\n    # Apply validation transforms (resize + normalize + to tensor)\n    transform = get_val_transforms(img_size)\n    augmented = transform(image=image_np)\n    tensor = augmented['image'].unsqueeze(0).to(device)  # add batch dim: (1, 3, H, W)\n\n    model.eval()\n    with autocast(enabled=CFG['fp16']):\n        out = model(pixel_values=tensor)\n\n    # Upsample logits to target size\n    logits = F.interpolate(out.logits, size=(img_size, img_size),\n                           mode='bilinear', align_corners=False)\n\n    pred = logits.argmax(dim=1).squeeze().cpu().numpy()  # class with highest score\n    probs = torch.softmax(logits, dim=1).squeeze().cpu().numpy()  # confidence scores\n    return pred, probs\n\n\n@torch.no_grad()\ndef tta_predict(model, image_np, img_size, device, scales=[0.75, 1.0, 1.25],\n                flips=[False, True]):\n    \"\"\"Test-Time Augmentation (TTA) \u2014 run inference multiple times with\n    different augmentations and AVERAGE the results for better accuracy.\n\n    For each combination of scale and flip:\n      1. Resize image to (scale * img_size)\n      2. Optionally flip horizontally\n      3. Run model \u2192 get softmax probabilities\n      4. Un-flip if needed\n      5. Resize probabilities back to img_size\n      6. Accumulate\n\n    Finally, average all accumulated probabilities and take argmax.\n    3 scales \u00d7 2 flips = 6 forward passes per image (slower but more accurate).\"\"\"\n\n    model.eval()\n    # Accumulator for softmax probabilities across all augmented versions\n    accum = np.zeros((CFG['num_classes'], img_size, img_size), dtype=np.float32)\n    count = 0  # how many versions we've accumulated\n\n    for scale in scales:        # e.g., [0.75, 1.0, 1.25]\n        sh, sw = int(img_size * scale), int(img_size * scale)  # scaled dimensions\n        for flip in flips:      # [False, True]\n            # Build transform for this specific scale + flip combo\n            tfm = A.Compose([\n                A.Resize(height=sh, width=sw),                         # resize\n                A.HorizontalFlip(p=1.0 if flip else 0.0),             # flip or not\n                A.Normalize(mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225]),                # ImageNet normalize\n                ToTensorV2(),                                           # to tensor\n            ])\n            aug = tfm(image=image_np)\n            tensor = aug['image'].unsqueeze(0).to(device)  # (1, 3, sh, sw)\n\n            # Run the model\n            with autocast(enabled=CFG['fp16']):\n                out = model(pixel_values=tensor)\n\n            # Resize logits to original img_size (all scales \u2192 same output size)\n            logits = F.interpolate(\n                out.logits, size=(img_size, img_size),\n                mode='bilinear', align_corners=False\n            )\n            # Convert logits to probabilities (0-1)\n            probs = torch.softmax(logits, dim=1).squeeze().cpu().numpy()\n\n            # If we flipped the image, flip the probabilities back\n            if flip:\n                probs = probs[:, :, ::-1].copy()\n\n            accum += probs  # add to running total\n            count += 1\n\n    # Average across all augmented versions\n    avg_probs = accum / count\n    # Final prediction = class with highest average probability\n    pred = np.argmax(avg_probs, axis=0).astype(np.uint8)\n    return pred, avg_probs\n\n\nprint('Training/validation/inference functions defined.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 11 \u2014 Training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TRAINING LOOP \u2014 The main training process\n# ============================================================\n# This is where the model actually LEARNS. Each epoch:\n#   1. Train on all training images (forward pass \u2192 loss \u2192 backward pass \u2192 update weights)\n#   2. Validate on held-out val set (check if we're improving)\n#   3. Save checkpoints (so we don't lose progress if Colab crashes)\n\n# --- Check for existing checkpoint to resume from ---\n# If we already trained before and saved a checkpoint, load it and continue\nckpt_path = os.path.join(CFG['save_dir'], 'best_model.pth')\n\n# History tracks all metrics across epochs (for plotting later)\nhistory = {\n    'train_loss': [], 'val_loss': [],\n    'val_miou': [], 'val_mdice': [], 'val_acc': [],\n    'lr': [],\n}\nstart_epoch = 0  # which epoch to start from\n\nif os.path.exists(ckpt_path):\n    # Load saved checkpoint: model weights + training history\n    ckpt = torch.load(ckpt_path, map_location=DEVICE, weights_only=False)\n    model.load_state_dict(ckpt['model_state_dict'])  # restore model weights\n    # Restore training history so plots show the full curve\n    saved_hist = ckpt.get('history', {})\n    for k in history:\n        history[k] = list(saved_hist.get(k, []))\n    start_epoch = ckpt.get('epoch', len(history['train_loss']))\n    print(f'Resumed from checkpoint: epoch {start_epoch}, val mIoU = {ckpt.get(\"miou\", 0):.4f}')\nelse:\n    print('No checkpoint found \u2014 training from scratch')\n\n# ============================================================\n# OPTIMIZER \u2014 Controls how weights get updated\n# ============================================================\n# We use DIFFERENTIAL learning rates:\n#   - Backbone (encoder): small LR because it's already pretrained\n#   - Decoder: bigger LR because it was randomly initialized\n\n# Separate parameters into backbone vs decoder groups\nbackbone_params = []\ndecoder_params = []\nfor name, param in model.named_parameters():\n    if 'decode_head' in name:\n        decoder_params.append(param)  # decoder layers\n    else:\n        backbone_params.append(param)  # encoder (backbone) layers\n\n# If we're resuming from a late epoch, use smaller LR (fine-tuning phase)\nif start_epoch >= 40:\n    backbone_lr = 1e-6   # very small \u2014 just minor adjustments\n    decoder_lr = 1e-5\nelse:\n    backbone_lr = CFG['backbone_lr']  # 6e-5\n    decoder_lr = CFG['decoder_lr']    # 6e-4\n\n# AdamW optimizer: Adam + weight decay (L2 regularization)\noptimizer = torch.optim.AdamW([\n    {'params': backbone_params, 'lr': backbone_lr},   # low LR for pretrained encoder\n    {'params': decoder_params, 'lr': decoder_lr},      # higher LR for decoder\n], weight_decay=CFG['weight_decay'])\n\nprint(f'Backbone params: {sum(p.numel() for p in backbone_params):,} (lr={backbone_lr})')\nprint(f'Decoder params:  {sum(p.numel() for p in decoder_params):,} (lr={decoder_lr})')\n\n# ============================================================\n# LEARNING RATE SCHEDULER \u2014 Changes LR during training\n# ============================================================\n# Schedule: warmup (LR ramps up) \u2192 cosine decay (LR gradually decreases)\n# Why? Starting with high LR can destabilize training. Ending with low LR\n# helps the model converge to a better minimum.\n\nremaining_epochs = 20  # how many more epochs to train\ntotal_epochs = start_epoch + remaining_epochs\n\nrem_total_steps = len(train_loader) * remaining_epochs  # total optimizer steps\nrem_warmup = int(rem_total_steps * 0.1)  # 10% of steps are warmup\n\ndef lr_lambda(step):\n    \"\"\"Compute the LR multiplier for the current step.\n    - During warmup: linearly increase from 0 to 1\n    - After warmup: cosine decay from 1 down to 0.05\"\"\"\n    if step < rem_warmup:\n        return float(step) / float(max(1, rem_warmup))  # linear warmup\n    # Cosine decay: smoothly decreases LR following a cosine curve\n    progress = float(step - rem_warmup) / float(max(1, rem_total_steps - rem_warmup))\n    return max(0.05, 0.5 * (1.0 + np.cos(np.pi * progress)))  # min 5% of original LR\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n# GradScaler for FP16 training \u2014 scales gradients to prevent underflow\nscaler = GradScaler(enabled=CFG['fp16'])\n\n# Metrics tracker for validation\nmetrics = SegmentationMetrics(CFG['num_classes'])\n\n# Track the best validation mIoU (for saving the best checkpoint)\nbest_val_miou = max(history['val_miou']) if history['val_miou'] else 0.0\n\n# ============================================================\n# THE ACTUAL TRAINING LOOP\n# ============================================================\nprint(f'\\n{\"=\" * 70}')\nprint(f'Training epochs {start_epoch+1}\u2013{total_epochs} on {len(combined_train_ds)} images')\nprint(f'(original: {len(original_train_ds)} + test samples: {len(sampled_test_ds)})')\nprint(f'{\"=\" * 70}')\n\nfor ep in range(1, remaining_epochs + 1):\n    disp = start_epoch + ep  # display epoch number (accounting for resumed training)\n    print(f'\\nEpoch {disp}/{total_epochs}')\n\n    # --- TRAIN for one epoch ---\n    train_loss = train_one_epoch(\n        model, train_loader, criterion, optimizer, scheduler,\n        scaler, grad_accum_steps=1, device=DEVICE, fp16=CFG['fp16']\n    )\n\n    # --- VALIDATE on val set (check if model improved) ---\n    val_loss, miou, mdice, acc, per_class_iou = validate(\n        model, val_loader, criterion, metrics, DEVICE\n    )\n\n    # Get current learning rate (for logging)\n    current_lr = optimizer.param_groups[0]['lr']\n\n    # Record everything in history (for plotting later)\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['val_miou'].append(miou)\n    history['val_mdice'].append(mdice)\n    history['val_acc'].append(acc)\n    history['lr'].append(current_lr)\n\n    # Print epoch summary\n    print(f'  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n    print(f'  Val mIoU: {miou:.4f} | mDice: {mdice:.4f} | Acc: {acc:.4f} | LR: {current_lr:.2e}')\n\n    # Print per-class IoU (abbreviated names)\n    iou_str = ' | '.join([f'{CLASS_NAMES[i][:4]}:{per_class_iou[i]:.3f}'\n                          for i in range(CFG['num_classes'])])\n    print(f'  IoU: {iou_str}')\n\n    # --- SAVE BEST MODEL (if this epoch has the highest val mIoU so far) ---\n    if miou > best_val_miou:\n        best_val_miou = miou\n        torch.save({\n            'epoch': disp,\n            'model_state_dict': model.state_dict(),  # the trained weights\n            'miou': miou,\n            'history': history,\n        }, os.path.join(CFG['save_dir'], 'best_model.pth'))\n        print(f'  >>> New best val mIoU: {best_val_miou:.4f} \u2014 saved!')\n\n    # --- SAVE PERIODIC CHECKPOINT (every 5 epochs) ---\n    if disp % CFG['save_every'] == 0:\n        torch.save({\n            'epoch': disp,\n            'model_state_dict': model.state_dict(),\n            'miou': miou,\n            'history': history,\n        }, os.path.join(CFG['save_dir'], f'checkpoint_epoch{disp}.pth'))\n\n    # --- ALWAYS SAVE LATEST (in case Colab crashes, we can resume) ---\n    torch.save({\n        'epoch': disp,\n        'model_state_dict': model.state_dict(),\n        'miou': miou,\n        'history': history,\n    }, os.path.join(CFG['save_dir'], 'latest_model.pth'))\n\n# ============================================================\n# TRAINING COMPLETE \u2014 Print summary\n# ============================================================\nprint(f'\\n{\"=\" * 70}')\nprint(f'Training complete! Total epochs: {total_epochs}')\nprint(f'Best val mIoU: {best_val_miou:.4f}')\nprint(f'Final val mIoU: {history[\"val_miou\"][-1]:.4f}')\nprint(f'{\"=\" * 70}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 12 \u2014 Training Curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TRAINING CURVES \u2014 Plot how loss and metrics changed over training\n# ============================================================\n# These plots help us understand:\n#   - Is the model still learning? (loss should decrease)\n#   - Is it overfitting? (train loss drops but val loss rises)\n#   - When was the best epoch? (highest val mIoU)\n\n# --- Load history from checkpoints ---\n# We stitch together history from phase 1 (original training) and phase 2 (fine-tuning)\nckpt1_path = os.path.join(CFG['save_dir'], 'best_model.pth')\nckpt2_path = os.path.join(CFG['save_dir'], 'latest_model_ft.pth')\n\nh1 = torch.load(ckpt1_path, map_location='cpu', weights_only=False).get('history', {})\nh2 = torch.load(ckpt2_path, map_location='cpu', weights_only=False).get('ft_history', {})\n\n# Combine both phases into one continuous history\nhistory = {\n    'train_loss': h1.get('train_loss', []) + h2.get('train_loss', []),\n    'val_loss':   h1.get('val_loss', [])   + h2.get('val_loss', []),\n    'val_miou':   h1.get('miou', [])       + h2.get('val_miou', []),\n    'lr':         h1.get('lr', [])         + h2.get('lr', []),\n}\n\n# Some metrics only exist in phase 1 \u2014 pad phase 2 with last known value\nif h1.get('mdice'):\n    last_mdice = h1['mdice'][-1]\n    history['val_mdice'] = h1['mdice'] + [last_mdice] * len(h2.get('val_miou', []))\n\nif h1.get('pixel_acc'):\n    last_acc = h1['pixel_acc'][-1]\n    history['val_acc'] = h1['pixel_acc'] + [last_acc] * len(h2.get('val_miou', []))\n\nif h1.get('per_class_iou'):\n    last_pci = h1['per_class_iou'][-1]\n    history['per_class_iou'] = h1['per_class_iou'] + [last_pci] * len(h2.get('val_miou', []))\n\nn_epochs = len(history['train_loss'])\nphase1_epochs = len(h1.get('train_loss', []))\nphase2_epochs = len(h2.get('train_loss', []))\n\nprint(f'Combined history: {n_epochs} epochs ({phase1_epochs} + {phase2_epochs})')\nprint(f'Train loss: {history[\"train_loss\"][0]:.4f} \u2192 {history[\"train_loss\"][-1]:.4f}')\nprint(f'Val loss:   {history[\"val_loss\"][0]:.4f} \u2192 {history[\"val_loss\"][-1]:.4f}')\nprint(f'Val mIoU:   {history[\"val_miou\"][0]:.4f} \u2192 {history[\"val_miou\"][-1]:.4f} (best: {max(history[\"val_miou\"]):.4f})')\n\n# ======================== MAIN 2x2 PLOT ========================\nepochs_range = range(1, n_epochs + 1)\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Top-left: Training loss (should decrease over time)\naxes[0, 0].plot(epochs_range, history['train_loss'], 'b-', linewidth=1.2)\naxes[0, 0].set_title('Training Loss')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Top-right: Validation loss (should decrease; if it rises = overfitting)\naxes[0, 1].plot(epochs_range, history['val_loss'], 'r-', linewidth=1.2)\naxes[0, 1].set_title('Validation Loss')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Loss')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Bottom-left: Validation mIoU (THE key metric \u2014 higher is better)\naxes[1, 0].plot(epochs_range, history['val_miou'], 'g-', linewidth=1.2)\nbest_ep = int(np.argmax(history['val_miou'])) + 1  # epoch with best mIoU\nbest_miou_val = max(history['val_miou'])\naxes[1, 0].axhline(y=best_miou_val, color='gray', linestyle='--', alpha=0.5,\n                    label=f'Best: {best_miou_val:.4f} (epoch {best_ep})')\naxes[1, 0].scatter([best_ep], [best_miou_val], color='red', s=60, zorder=5)  # mark best point\naxes[1, 0].set_title('Validation mIoU')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('mIoU')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Bottom-right: Learning rate schedule (shows warmup \u2192 cosine decay)\naxes[1, 1].plot(epochs_range, history['lr'], 'purple', linewidth=1.2)\naxes[1, 1].set_title('Learning Rate (Backbone)')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('LR')\naxes[1, 1].set_yscale('log')  # log scale since LR spans orders of magnitude\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.suptitle(f'SegFormer-B4 Training \u2014 {n_epochs} Epochs', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig(os.path.join(CFG['save_dir'], 'training_curves.png'), dpi=150)\nplt.show()\n\n# ======================== mDice + Pixel Accuracy (bonus plots) ========================\nif 'val_mdice' in history and 'val_acc' in history:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Mean Dice score over epochs\n    ax1.plot(epochs_range, history['val_mdice'], 'orange', linewidth=1.2)\n    best_dice_ep = int(np.argmax(history['val_mdice'])) + 1\n    ax1.scatter([best_dice_ep], [max(history['val_mdice'])], color='red', s=60, zorder=5)\n    ax1.set_title(f'Validation mDice (best: {max(history[\"val_mdice\"]):.4f})')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('mDice')\n    ax1.grid(True, alpha=0.3)\n\n    # Pixel accuracy over epochs\n    ax2.plot(epochs_range, history['val_acc'], 'teal', linewidth=1.2)\n    best_acc_ep = int(np.argmax(history['val_acc'])) + 1\n    ax2.scatter([best_acc_ep], [max(history['val_acc'])], color='red', s=60, zorder=5)\n    ax2.set_title(f'Validation Pixel Accuracy (best: {max(history[\"val_acc\"]):.4f})')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(CFG['save_dir'], 'training_curves_extra.png'), dpi=150)\n    plt.show()\n\n# ======================== Per-Class IoU over epochs ========================\n# Shows how each class's IoU evolves \u2014 helps spot which classes are hardest\nif 'per_class_iou' in history:\n    fig, ax = plt.subplots(figsize=(14, 6))\n    pci = np.array(history['per_class_iou'])  # shape: (n_epochs, 10)\n    for c in range(CFG['num_classes']):\n        ax.plot(epochs_range, pci[:, c], color=CLASS_COLORS[c] / 255.0,\n                linewidth=1.5, label=CLASS_NAMES[c])\n    ax.plot(epochs_range, history['val_miou'], 'k--', linewidth=2, label='Mean IoU')\n    ax.set_title('Per-Class IoU Over Training')\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel('IoU')\n    ax.set_ylim(0, 1)\n    ax.legend(loc='lower right', fontsize=8, ncol=2)\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(os.path.join(CFG['save_dir'], 'per_class_iou_over_epochs.png'), dpi=150)\n    plt.show()\n\n# ======================== Train vs Val Loss (overfitting check) ========================\n# If the gap between train and val loss grows = model is overfitting\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(epochs_range, history['train_loss'], 'b-', linewidth=1.2, label='Train Loss')\nax.plot(epochs_range, history['val_loss'], 'r-', linewidth=1.2, label='Val Loss')\nax.fill_between(epochs_range, history['train_loss'], history['val_loss'],\n                alpha=0.1, color='gray')  # shaded gap between train and val\nax.set_title('Train vs Validation Loss')\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(CFG['save_dir'], 'train_vs_val_loss.png'), dpi=150)\nplt.show()\n\nprint(f'\\nAll training curves saved to {CFG[\"save_dir\"]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 13 \u2014 Test Set Evaluation with TTA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TEST SET EVALUATION \u2014 Run predictions on ALL ~1002 test images\n# ============================================================\n# This is the FINAL evaluation: how well does the model do on images\n# it has never seen during training? We use TTA for best accuracy.\n\n# --- Load the best available checkpoint ---\nlatest_path = os.path.join(CFG['save_dir'], 'latest_model_ft.pth')\nbest_path = os.path.join(CFG['save_dir'], 'best_model.pth')\n\nif os.path.exists(latest_path):\n    ckpt_eval = torch.load(latest_path, map_location=DEVICE, weights_only=False)\n    model.load_state_dict(ckpt_eval['model_state_dict'])\n    print(f'Loaded checkpoint (epoch {ckpt_eval.get(\"epoch\", \"?\")}, val mIoU={ckpt_eval.get(\"miou\", 0):.4f})')\nelif os.path.exists(best_path):\n    ckpt_eval = torch.load(best_path, map_location=DEVICE, weights_only=False)\n    model.load_state_dict(ckpt_eval['model_state_dict'])\n    print(f'Loaded checkpoint (epoch {ckpt_eval.get(\"epoch\", \"?\")}, val mIoU={ckpt_eval.get(\"miou\", 0):.4f})')\nelse:\n    print('Using current model weights')\n\n# --- Run TTA prediction on every test image ---\nprint(f'\\nEvaluating on {len(test_eval_imgs)} test images with TTA...')\ntest_metrics = SegmentationMetrics(CFG['num_classes'])\n\n# Create folders to save predictions (both raw class masks and colored visualizations)\nos.makedirs(os.path.join(CFG['output_dir'], 'colored'), exist_ok=True)  # colorful RGB masks\nos.makedirs(os.path.join(CFG['output_dir'], 'raw'), exist_ok=True)      # raw uint16 masks\n\nfor img_p, mask_p in tqdm(zip(test_eval_imgs, test_eval_masks),\n                          total=len(test_eval_imgs), desc='Test TTA'):\n    # Load the test image\n    img = np.array(Image.open(img_p).convert('RGB'))\n\n    # Run TTA prediction (3 scales x 2 flips = 6 forward passes)\n    pred, _ = tta_predict(model, img, CFG['img_size'], DEVICE)\n\n    # Resize prediction back to ORIGINAL image size (model works at 512x512)\n    orig_h, orig_w = img.shape[:2]\n    pred_resized = np.array(Image.fromarray(pred).resize(\n        (orig_w, orig_h), Image.NEAREST))  # NEAREST = no interpolation for class IDs\n\n    # --- Save raw prediction mask (uint16, same format as ground truth) ---\n    raw_mask = np.zeros_like(pred_resized, dtype=np.uint16)\n    for class_id, raw_val in REVERSE_MAP.items():\n        raw_mask[pred_resized == class_id] = raw_val  # convert class IDs back to raw values\n    Image.fromarray(raw_mask).save(\n        os.path.join(CFG['output_dir'], 'raw', os.path.basename(img_p)))\n\n    # --- Save colored prediction (RGB, for visual inspection) ---\n    Image.fromarray(colorize_mask(pred_resized)).save(\n        os.path.join(CFG['output_dir'], 'colored', os.path.basename(img_p)))\n\n    # --- Evaluate against ground truth ---\n    gt_raw = np.array(Image.open(mask_p))\n    gt = convert_mask(gt_raw)\n    test_metrics.update(pred_resized[np.newaxis], gt[np.newaxis])  # add batch dim\n\n# ============================================================\n# PRINT TEST RESULTS\n# ============================================================\nprint('\\n' + '=' * 60)\nprint('TEST SET EVALUATION (TTA)')\nprint('=' * 60)\n\ntest_iou, test_miou = test_metrics.get_iou()\ntest_dice, test_mdice = test_metrics.get_dice()\ntest_acc = test_metrics.get_pixel_accuracy()\n\n# Per-class breakdown\nprint(f'\\n{\"Class\":<20} {\"IoU\":>8} {\"Dice\":>8}')\nprint('-' * 38)\nfor i in range(CFG['num_classes']):\n    print(f'{CLASS_NAMES[i]:<20} {test_iou[i]:>8.4f} {test_dice[i]:>8.4f}')\nprint('-' * 38)\nprint(f'{\"Mean\":<20} {test_miou:>8.4f} {test_mdice:>8.4f}')\nprint(f'\\nPixel Accuracy: {test_acc:.4f}')\nprint(f'Mean IoU:       {test_miou:.4f} ({test_miou*100:.2f}%)')\nprint(f'Mean Dice:      {test_mdice:.4f}')\nprint(f'\\nPredictions saved to {CFG[\"output_dir\"]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 14 \u2014 Visualizations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# VISUALIZATIONS \u2014 Charts and sample predictions\n# ============================================================\n\n# ======================== 1. Per-Class IoU Bar Chart ========================\n# Shows which classes the model is best/worst at segmenting\n\nfig, ax = plt.subplots(figsize=(14, 6))\ncolors = [CLASS_COLORS[i] / 255.0 for i in range(CFG['num_classes'])]\nbars = ax.bar(CLASS_NAMES, test_iou, color=colors, edgecolor='black', linewidth=0.5)\nax.set_ylabel('IoU')\nax.set_title(f'Per-Class IoU on Test Set (mIoU = {test_miou:.4f})')\nax.set_ylim(0, 1)\nax.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on top of each bar\nfor bar, val in zip(bars, test_iou):\n    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,\n            f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n\nplt.xticks(rotation=30, ha='right')\nplt.tight_layout()\nplt.savefig(os.path.join(CFG['save_dir'], 'test_per_class_iou.png'), dpi=150)\nplt.show()\n\n# ======================== 2. Confusion Matrix Heatmap ========================\n# Shows which classes get confused with each other.\n# Rows = true class, Columns = predicted class.\n# Diagonal = correct predictions. Off-diagonal = mistakes.\n# e.g., if (row=Dry Grass, col=Landscape) is high, the model confuses them.\n\ncm = test_metrics.confusion_matrix.astype(np.float32)\n# Normalize each row to percentages (what fraction of true class X got predicted as Y)\ncm_norm = cm / (cm.sum(axis=1, keepdims=True) + 1e-6)\n\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=ax)\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nax.set_title(f'Test Set Confusion Matrix (mIoU = {test_miou:.4f})')\nplt.tight_layout()\nplt.savefig(os.path.join(CFG['save_dir'], 'test_confusion_matrix.png'), dpi=150)\nplt.show()\n\n# ======================== 3. Sample Predictions ========================\n# Side-by-side comparison: Original Image | Model's Prediction | Ground Truth\n# This is the most intuitive way to see how well the model performs.\n\nn_vis = min(10, len(test_eval_imgs))  # show up to 10 random samples\nvis_idx = random.sample(range(len(test_eval_imgs)), n_vis)\n\nfig, axes = plt.subplots(n_vis, 3, figsize=(15, 4 * n_vis))\nfor i, idx in enumerate(vis_idx):\n    # Load image and run prediction\n    img = np.array(Image.open(test_eval_imgs[idx]).convert('RGB'))\n    pred, _ = predict_single(model, img, CFG['img_size'], DEVICE)\n\n    # Load and process ground truth mask\n    gt_raw = np.array(Image.open(test_eval_masks[idx]))\n    gt = convert_mask(gt_raw)\n    gt_r = np.array(Image.fromarray(gt).resize(\n        (CFG['img_size'], CFG['img_size']), Image.NEAREST))\n\n    # Column 1: Original input image\n    axes[i, 0].imshow(img)\n    axes[i, 0].set_title(os.path.basename(test_eval_imgs[idx]), fontsize=9)\n    axes[i, 0].axis('off')\n\n    # Column 2: Model's prediction (colorized)\n    axes[i, 1].imshow(colorize_mask(pred))\n    axes[i, 1].set_title('Prediction', fontsize=9)\n    axes[i, 1].axis('off')\n\n    # Column 3: Ground truth (colorized)\n    axes[i, 2].imshow(colorize_mask(gt_r))\n    axes[i, 2].set_title('Ground Truth', fontsize=9)\n    axes[i, 2].axis('off')\n\n# Add color legend at the bottom so viewers know which color = which class\npatches = [mpatches.Patch(color=CLASS_COLORS[c] / 255.0, label=CLASS_NAMES[c])\n           for c in range(CFG['num_classes'])]\nfig.legend(handles=patches, loc='lower center', ncol=5, fontsize=9)\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.04)\nplt.savefig(os.path.join(CFG['save_dir'], 'test_visualizations.png'), dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 15 \u2014 Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# FINAL SUMMARY \u2014 Everything in one place\n# ============================================================\n# This cell prints a clean summary of the entire experiment:\n# training config, best metrics, per-class results, and saved files.\n\nn_total_epochs = len(history['train_loss'])\nbest_val_ep = int(np.argmax(history['val_miou'])) + 1  # which epoch had best val mIoU\nbest_val = max(history['val_miou'])\nfinal_val = history['val_miou'][-1]\n\nprint('=' * 70)\nprint('FINAL SUMMARY \u2014 Desert Segmentation (SegFormer-B4)')\nprint('=' * 70)\nprint()\n\n# --- Model and training config ---\nprint(f'Model:            SegFormer-B4 ({CFG[\"model_name\"]})')\nprint(f'Image size:       {CFG[\"img_size\"]}x{CFG[\"img_size\"]}')\nprint(f'Total epochs:     {n_total_epochs}')\nprint(f'Loss:             0.5*Focal + 0.3*Dice + 0.2*WeightedCE')\nprint()\n\n# --- Key metrics ---\nprint(f'{\"Metric\":<25} {\"Value\":>10}')\nprint('-' * 37)\nprint(f'{\"Best val mIoU\":<25} {best_val*100:>9.2f}%  (epoch {best_val_ep})')\nprint(f'{\"Final val mIoU\":<25} {final_val*100:>9.2f}%')\nprint(f'{\"Test mIoU (TTA)\":<25} {test_miou*100:>9.2f}%')    # the number that matters most\nprint(f'{\"Test mDice (TTA)\":<25} {test_mdice*100:>9.2f}%')\nprint(f'{\"Test Pixel Acc\":<25} {test_acc*100:>9.2f}%')\nif 'val_mdice' in history:\n    print(f'{\"Best val mDice\":<25} {max(history[\"val_mdice\"])*100:>9.2f}%')\nif 'val_acc' in history:\n    print(f'{\"Best val Pixel Acc\":<25} {max(history[\"val_acc\"])*100:>9.2f}%')\nprint()\n\n# --- Per-class test results ---\nprint(f'{\"Class\":<20} {\"Test IoU\":>10} {\"Test Dice\":>11}')\nprint('-' * 43)\nfor i in range(CFG['num_classes']):\n    print(f'{CLASS_NAMES[i]:<20} {test_iou[i]*100:>9.2f}% {test_dice[i]*100:>10.2f}%')\nprint('-' * 43)\nprint(f'{\"Mean\":<20} {test_miou*100:>9.2f}% {test_mdice*100:>10.2f}%')\nprint()\n\n# --- Saved checkpoints ---\nprint('Checkpoints saved to Google Drive:')\nfor name in ['best_model.pth', 'latest_model_ft.pth']:\n    path = os.path.join(CFG['save_dir'], name)\n    if os.path.exists(path):\n        ck = torch.load(path, map_location='cpu', weights_only=False)\n        print(f'  [{name}] epoch {ck.get(\"epoch\",\"?\")}, val mIoU = {ck.get(\"miou\",0):.4f}')\n    else:\n        print(f'  [{name}] not found')\n\nprint(f'\\nPredictions saved to: {CFG[\"output_dir\"]}')"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
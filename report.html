<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Semantic Segmentation for Desert UGV Navigation | Hackathon Submission</title>
<style>
  /* ===== Reset & Base ===== */
  *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

  html { scroll-behavior: smooth; }

  body {
    font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
    line-height: 1.7;
    color: #1F2937;
    background: #ffffff;
    -webkit-font-smoothing: antialiased;
  }

  /* ===== Layout ===== */
  .container {
    max-width: 900px;
    margin: 0 auto;
    padding: 40px 24px 80px;
  }

  /* ===== Title Section ===== */
  .title-section {
    text-align: center;
    padding: 60px 0 48px;
    border-bottom: 3px solid #2563EB;
    margin-bottom: 48px;
  }

  .title-section h1 {
    font-size: 2.4rem;
    font-weight: 800;
    color: #111827;
    letter-spacing: -0.02em;
    margin-bottom: 12px;
  }

  .title-section .subtitle {
    font-size: 1.15rem;
    font-weight: 500;
    color: #2563EB;
    letter-spacing: 0.04em;
  }

  .title-section .team-info {
    margin-top: 18px;
    font-size: 0.95rem;
    color: #6B7280;
  }

  /* ===== Table of Contents ===== */
  .toc {
    background: #F9FAFB;
    border: 1px solid #E5E7EB;
    border-radius: 10px;
    padding: 28px 32px;
    margin-bottom: 48px;
  }

  .toc h2 {
    font-size: 1.2rem;
    font-weight: 700;
    color: #111827;
    margin-bottom: 16px;
    border: none;
    padding: 0;
  }

  .toc ol {
    list-style: none;
    counter-reset: toc-counter;
    padding: 0;
    columns: 2;
    column-gap: 32px;
  }

  .toc ol li {
    counter-increment: toc-counter;
    margin-bottom: 8px;
    break-inside: avoid;
  }

  .toc ol li::before {
    content: counter(toc-counter) ". ";
    font-weight: 600;
    color: #2563EB;
  }

  .toc a {
    color: #374151;
    text-decoration: none;
    font-size: 0.95rem;
    transition: color 0.15s;
  }

  .toc a:hover { color: #2563EB; text-decoration: underline; }

  /* ===== Section Headers ===== */
  h2.section-title {
    font-size: 1.55rem;
    font-weight: 700;
    color: #111827;
    border-left: 4px solid #2563EB;
    padding-left: 16px;
    margin-top: 56px;
    margin-bottom: 24px;
  }

  h3 {
    font-size: 1.15rem;
    font-weight: 600;
    color: #1F2937;
    margin-top: 28px;
    margin-bottom: 12px;
  }

  /* ===== Paragraphs ===== */
  p {
    margin-bottom: 14px;
    font-size: 0.97rem;
    color: #374151;
  }

  /* ===== Metric Callout Cards ===== */
  .metric-cards {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
    gap: 16px;
    margin: 24px 0;
  }

  .metric-card {
    background: #F0FDF4;
    border: 1px solid #BBF7D0;
    border-radius: 10px;
    padding: 20px 18px;
    text-align: center;
  }

  .metric-card .metric-value {
    font-size: 1.8rem;
    font-weight: 800;
    color: #059669;
    display: block;
  }

  .metric-card .metric-label {
    font-size: 0.82rem;
    font-weight: 500;
    color: #6B7280;
    margin-top: 4px;
    text-transform: uppercase;
    letter-spacing: 0.05em;
  }

  .metric-card.blue {
    background: #EFF6FF;
    border-color: #BFDBFE;
  }

  .metric-card.blue .metric-value { color: #2563EB; }

  /* ===== Tables ===== */
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 20px 0;
    font-size: 0.92rem;
  }

  thead th {
    background: #1F2937;
    color: #ffffff;
    font-weight: 600;
    padding: 12px 14px;
    text-align: left;
    font-size: 0.85rem;
    text-transform: uppercase;
    letter-spacing: 0.04em;
  }

  thead th:first-child { border-radius: 6px 0 0 0; }
  thead th:last-child { border-radius: 0 6px 0 0; }

  tbody td {
    padding: 10px 14px;
    border-bottom: 1px solid #E5E7EB;
    vertical-align: middle;
  }

  tbody tr:nth-child(even) { background: #F9FAFB; }
  tbody tr:hover { background: #F3F4F6; }

  .color-swatch {
    display: inline-block;
    width: 18px;
    height: 18px;
    border-radius: 4px;
    vertical-align: middle;
    margin-right: 8px;
    border: 1px solid rgba(0,0,0,0.12);
  }

  /* ===== Figures & Images ===== */
  .figure {
    margin: 28px 0;
    text-align: center;
  }

  .figure img {
    max-width: 100%;
    height: auto;
    border-radius: 8px;
    box-shadow: 0 2px 12px rgba(0,0,0,0.08);
  }

  .figure figcaption, .figure .caption {
    font-size: 0.85rem;
    color: #6B7280;
    margin-top: 10px;
    font-style: italic;
  }

  /* ===== Formula Blocks ===== */
  .formula-block {
    background: #F3F4F6;
    border: 1px solid #D1D5DB;
    border-radius: 8px;
    padding: 18px 22px;
    margin: 18px 0;
    font-family: 'Courier New', Courier, monospace;
    font-size: 0.93rem;
    line-height: 1.9;
    overflow-x: auto;
  }

  /* ===== Code Blocks ===== */
  code {
    background: #F3F4F6;
    padding: 2px 6px;
    border-radius: 4px;
    font-size: 0.88rem;
    font-family: 'SF Mono', 'Fira Code', 'Courier New', monospace;
  }

  /* ===== Highlight Box ===== */
  .highlight-box {
    background: #EFF6FF;
    border-left: 4px solid #2563EB;
    border-radius: 0 8px 8px 0;
    padding: 16px 20px;
    margin: 20px 0;
    font-size: 0.95rem;
  }

  .highlight-box strong { color: #1E40AF; }

  .highlight-box.green {
    background: #F0FDF4;
    border-left-color: #059669;
  }

  .highlight-box.green strong { color: #047857; }

  /* ===== Architecture Diagram (HTML/CSS) ===== */
  .arch-diagram {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 0;
    margin: 28px 0;
    flex-wrap: wrap;
  }

  .arch-box {
    background: #ffffff;
    border: 2px solid #2563EB;
    border-radius: 8px;
    padding: 14px 18px;
    text-align: center;
    min-width: 120px;
    font-size: 0.82rem;
    font-weight: 600;
    color: #1E40AF;
  }

  .arch-box.green { border-color: #059669; color: #047857; }
  .arch-box.gray { border-color: #6B7280; color: #374151; }
  .arch-box.purple { border-color: #7C3AED; color: #6D28D9; }

  .arch-box small {
    display: block;
    font-weight: 400;
    color: #6B7280;
    font-size: 0.75rem;
    margin-top: 4px;
  }

  .arch-arrow {
    font-size: 1.4rem;
    color: #9CA3AF;
    margin: 0 6px;
    flex-shrink: 0;
  }

  /* ===== Lists ===== */
  ul, ol { margin: 12px 0 12px 24px; }
  li { margin-bottom: 6px; font-size: 0.95rem; }

  /* ===== Divider ===== */
  hr {
    border: none;
    border-top: 1px solid #E5E7EB;
    margin: 40px 0;
  }

  /* ===== Print Styles ===== */
  @media print {
    body { font-size: 11pt; }
    .container { max-width: 100%; padding: 20px; }
    .title-section { padding: 30px 0 24px; }
    h2.section-title { break-before: page; margin-top: 24px; }
    .metric-cards { break-inside: avoid; }
    table { break-inside: avoid; }
    .figure { break-inside: avoid; }
    .toc { break-inside: avoid; }
    .figure img { box-shadow: none; border: 1px solid #ccc; }
    .arch-diagram { break-inside: avoid; }
  }
</style>
</head>
<body>
<div class="container">

  <!-- ============================================================ -->
  <!-- TITLE SECTION -->
  <!-- ============================================================ -->
  <div class="title-section">
    <h1>Semantic Segmentation for Desert UGV Navigation</h1>
    <div class="subtitle">SegFormer-B4 | Hackathon Submission</div>
    <div class="team-info">Pixel-level Scene Understanding for Autonomous Off-Road Vehicles</div>
  </div>

  <!-- ============================================================ -->
  <!-- TABLE OF CONTENTS -->
  <!-- ============================================================ -->
  <nav class="toc">
    <h2>Table of Contents</h2>
    <ol>
      <li><a href="#problem-statement">Problem Statement</a></li>
      <li><a href="#dataset-overview">Dataset Overview</a></li>
      <li><a href="#approach-model">Approach &amp; Model Selection</a></li>
      <li><a href="#data-augmentation">Data Augmentation Pipeline</a></li>
      <li><a href="#loss-function">Loss Function Design</a></li>
      <li><a href="#training-config">Training Configuration</a></li>
      <li><a href="#training-results">Training Results &amp; Analysis</a></li>
      <li><a href="#tta-inference">Test-Time Augmentation &amp; Inference</a></li>
      <li><a href="#test-evaluation">Test Set Evaluation</a></li>
      <li><a href="#demo-architecture">Real-Time Demo Architecture</a></li>
      <li><a href="#challenges">Challenges &amp; Key Learnings</a></li>
      <li><a href="#conclusion">Conclusion &amp; Future Work</a></li>
    </ol>
  </nav>

  <!-- ============================================================ -->
  <!-- 1. PROBLEM STATEMENT -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="problem-statement">1. Problem Statement</h2>

  <p>
    Unmanned Ground Vehicles (UGVs) operating in desert environments require pixel-level scene understanding to navigate safely through challenging off-road terrain. Semantic segmentation provides this capability by classifying every pixel in the camera feed into meaningful environmental categories, enabling the vehicle to distinguish drivable surfaces from obstacles, vegetation, and other hazards in real time.
  </p>

  <p>
    A critical challenge arises from the use of digital twin simulations for training data generation. While synthetic datasets offer perfectly labeled segmentation masks at scale, models trained on one simulated desert must generalize to entirely unseen desert environments without memorizing scene-specific patterns. This domain generalization gap is the central technical hurdle of this project.
  </p>

  <p>
    The task involves classifying each pixel into one of 10 environmental classes: Trees, Lush Bushes, Dry Grass, Dry Bushes, Ground Clutter, Flowers, Logs, Rocks, Landscape, and Sky. The training data comes from one digital twin desert, while the test set comes from a completely different desert environment, directly testing domain transfer capability.
  </p>

  <!-- ============================================================ -->
  <!-- 2. DATASET OVERVIEW -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="dataset-overview">2. Dataset Overview</h2>

  <div class="metric-cards">
    <div class="metric-card blue">
      <span class="metric-value">2,858</span>
      <span class="metric-label">Image-Mask Pairs</span>
    </div>
    <div class="metric-card blue">
      <span class="metric-value">960 x 540</span>
      <span class="metric-label">Resolution (px)</span>
    </div>
    <div class="metric-card blue">
      <span class="metric-value">~2 GB</span>
      <span class="metric-label">Dataset Size</span>
    </div>
    <div class="metric-card blue">
      <span class="metric-value">10</span>
      <span class="metric-label">Semantic Classes</span>
    </div>
  </div>

  <p>
    The dataset is generated from a digital twin desert simulation, providing RGB images paired with pixel-accurate segmentation masks. Each mask encodes class membership through specific pixel intensity values that are mapped to semantic class IDs during preprocessing.
  </p>

  <h3>Class Definitions &amp; Color Palette</h3>
  <table>
    <thead>
      <tr>
        <th>Class</th>
        <th>Color</th>
        <th>RGB Value</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><span class="color-swatch" style="background:#228B22;"></span>Trees</td>
        <td>Dark Green</td>
        <td>(34, 139, 34)</td>
        <td>Large vegetation, canopy structures</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#00FF7F;"></span>Lush Bushes</td>
        <td>Spring Green</td>
        <td>(0, 255, 127)</td>
        <td>Dense green shrubs</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#BDB76B;"></span>Dry Grass</td>
        <td>Khaki</td>
        <td>(189, 183, 107)</td>
        <td>Dried ground-level grass</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#8B7765;"></span>Dry Bushes</td>
        <td>Tan</td>
        <td>(139, 119, 101)</td>
        <td>Dead or dried-out shrubs</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#A0522D;"></span>Ground Clutter</td>
        <td>Sienna</td>
        <td>(160, 82, 45)</td>
        <td>Misc. ground-level debris</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#FF69B4;"></span>Flowers</td>
        <td>Hot Pink</td>
        <td>(255, 105, 180)</td>
        <td>Small flowering plants (rare)</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#8B4513;"></span>Logs</td>
        <td>Saddle Brown</td>
        <td>(139, 69, 19)</td>
        <td>Fallen wood, branches (rare)</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#808080;"></span>Rocks</td>
        <td>Gray</td>
        <td>(128, 128, 128)</td>
        <td>Stones, boulders</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#D2B48C;"></span>Landscape</td>
        <td>Tan</td>
        <td>(210, 180, 140)</td>
        <td>General ground surface / terrain</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#87CEEB;"></span>Sky</td>
        <td>Sky Blue</td>
        <td>(135, 206, 235)</td>
        <td>Sky region (dominant class)</td>
      </tr>
    </tbody>
  </table>

  <h3>Pixel Value Mapping</h3>
  <p>
    Segmentation masks encode class labels as raw pixel intensity values. During preprocessing, these values are mapped to contiguous class IDs (0-9) for model training.
  </p>
  <table>
    <thead>
      <tr>
        <th>Raw Pixel Value</th>
        <th>Class ID</th>
        <th>Class Name</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>100</td><td>0</td><td>Trees</td></tr>
      <tr><td>200</td><td>1</td><td>Lush Bushes</td></tr>
      <tr><td>300</td><td>2</td><td>Dry Grass</td></tr>
      <tr><td>500</td><td>3</td><td>Dry Bushes</td></tr>
      <tr><td>550</td><td>4</td><td>Ground Clutter</td></tr>
      <tr><td>600</td><td>5</td><td>Flowers</td></tr>
      <tr><td>700</td><td>6</td><td>Logs</td></tr>
      <tr><td>800</td><td>7</td><td>Rocks</td></tr>
      <tr><td>7100</td><td>8</td><td>Landscape</td></tr>
      <tr><td>10000</td><td>9</td><td>Sky</td></tr>
    </tbody>
  </table>

  <h3>Data Splits</h3>
  <p>
    The dataset is divided into training (~2,400 images), validation (~458 images), and test sets. The test set originates from a different digital twin desert environment, ensuring that evaluation measures true domain generalization rather than scene memorization.
  </p>

  <!-- ============================================================ -->
  <!-- 3. APPROACH & MODEL SELECTION -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="approach-model">3. Approach &amp; Model Selection</h2>

  <p>
    After evaluating several architectures, we selected <strong>SegFormer-B4</strong> (NVIDIA <code>mit-b4</code>) as our primary model. This decision was informed by systematic benchmarking against alternative approaches and the specific requirements of desert scene segmentation.
  </p>

  <h3>Why SegFormer-B4?</h3>

  <div class="highlight-box">
    <strong>Key Decision:</strong> The provided DINOv2 baseline achieved only 24.78% mIoU with a frozen encoder. SegFormer-B4, being fully end-to-end trainable with hierarchical multi-scale features, was the strongest candidate for a domain-transfer task requiring fine-grained class discrimination.
  </div>

  <table>
    <thead>
      <tr>
        <th>Architecture</th>
        <th>Consideration</th>
        <th>Outcome</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>DINOv2 (Baseline)</strong></td>
        <td>Frozen encoder, linear head only</td>
        <td>24.78% mIoU -- insufficient for fine-grained classes</td>
      </tr>
      <tr>
        <td><strong>SAM</strong></td>
        <td>Class-agnostic segmentation, not designed for semantic seg</td>
        <td>Requires per-class prompting, poor for multi-class tasks</td>
      </tr>
      <tr>
        <td><strong>DeepLabV3+</strong></td>
        <td>Strong CNN-based option with atrous convolutions</td>
        <td>Good but lacks transformer-level global context</td>
      </tr>
      <tr>
        <td><strong>SegFormer-B4</strong></td>
        <td>Hierarchical ViT encoder + lightweight MLP decoder</td>
        <td>Selected -- best balance of accuracy, speed, and trainability</td>
      </tr>
    </tbody>
  </table>

  <h3>Architecture Key Advantages</h3>
  <ul>
    <li><strong>Multi-scale hierarchical features:</strong> 4-stage encoder produces feature maps at 1/4, 1/8, 1/16, and 1/32 resolution, capturing both fine details (flowers, logs) and global context (sky, landscape).</li>
    <li><strong>Lightweight MLP decoder:</strong> Aggregates multi-scale features efficiently without heavy computation, enabling fast inference.</li>
    <li><strong>End-to-end trainable:</strong> Unlike frozen-encoder approaches, the entire backbone adapts to desert domain features during training.</li>
    <li><strong>Fast inference:</strong> 30-40ms per image on M4 Pro (MPS), suitable for real-time UGV deployment.</li>
    <li><strong>~62M parameters:</strong> Large enough for high-capacity learning, yet trainable on a single T4 GPU with mixed precision.</li>
  </ul>

  <div class="figure">
    <img src="./report_assets/model_architecture.svg" alt="SegFormer-B4 Model Architecture">
    <div class="caption">Figure 1: SegFormer-B4 architecture showing the hierarchical encoder with 4-stage multi-scale feature extraction and the lightweight MLP decoder producing per-pixel class predictions.</div>
  </div>

  <h3>Architecture Overview</h3>
  <p>
    The encoder processes the input image through four transformer stages, each producing feature maps at progressively lower resolutions. Overlapping patch embeddings preserve spatial information at stage boundaries. The MLP decoder then fuses these multi-scale features through linear projections and upsampling, producing a dense prediction map with 10 class channels.
  </p>

  <div class="arch-diagram">
    <div class="arch-box">Input Image<small>512 x 512 x 3</small></div>
    <span class="arch-arrow">&#8594;</span>
    <div class="arch-box purple">Stage 1<small>128 x 128</small></div>
    <span class="arch-arrow">&#8594;</span>
    <div class="arch-box purple">Stage 2<small>64 x 64</small></div>
    <span class="arch-arrow">&#8594;</span>
    <div class="arch-box purple">Stage 3<small>32 x 32</small></div>
    <span class="arch-arrow">&#8594;</span>
    <div class="arch-box purple">Stage 4<small>16 x 16</small></div>
    <span class="arch-arrow">&#8594;</span>
    <div class="arch-box green">MLP Decoder<small>Fuse + Upsample</small></div>
    <span class="arch-arrow">&#8594;</span>
    <div class="arch-box gray">Output<small>512 x 512 x 10</small></div>
  </div>

  <!-- ============================================================ -->
  <!-- 4. DATA AUGMENTATION PIPELINE -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="data-augmentation">4. Data Augmentation Pipeline</h2>

  <p>
    A comprehensive data augmentation strategy is critical for domain generalization. We employ the Albumentations library to apply a diverse set of geometric and photometric transforms, simulating the visual variability the model will encounter in unseen desert environments.
  </p>

  <table>
    <thead>
      <tr>
        <th>Augmentation</th>
        <th>Parameters</th>
        <th>Purpose for Desert Segmentation</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>HorizontalFlip</strong></td>
        <td>p=0.5</td>
        <td>Desert scenes are horizontally symmetric; doubles effective data without artifacts</td>
      </tr>
      <tr>
        <td><strong>VerticalFlip</strong></td>
        <td>p=0.3</td>
        <td>Adds further spatial variation; sky/ground inversion forces robust features</td>
      </tr>
      <tr>
        <td><strong>RandomResizedCrop</strong></td>
        <td>512x512, scale 0.5-1.0</td>
        <td>Simulates different camera distances and field-of-view changes across terrains</td>
      </tr>
      <tr>
        <td><strong>ColorJitter</strong></td>
        <td>brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1</td>
        <td>Simulates lighting variation (dawn, dusk, overcast) across different deserts</td>
      </tr>
      <tr>
        <td><strong>GaussianBlur</strong></td>
        <td>kernel=5, sigma=0.1-2.0</td>
        <td>Mimics camera focus variation and atmospheric haze in desert environments</td>
      </tr>
      <tr>
        <td><strong>GridDistortion</strong></td>
        <td>p=0.3</td>
        <td>Simulates terrain elevation changes and camera lens distortion on uneven ground</td>
      </tr>
      <tr>
        <td><strong>CLAHE</strong></td>
        <td>clip_limit=2.0</td>
        <td>Enhances local contrast in sandy regions where classes blend together visually</td>
      </tr>
      <tr>
        <td><strong>RandomBrightnessContrast</strong></td>
        <td>p=0.5</td>
        <td>Handles exposure variation and strong sun/shadow transitions in desert scenes</td>
      </tr>
      <tr>
        <td><strong>RandomRotation</strong></td>
        <td>&plusmn;15 degrees</td>
        <td>Simulates camera tilt on uneven terrain during UGV navigation</td>
      </tr>
      <tr>
        <td><strong>RandomGrayscale</strong></td>
        <td>p=0.1</td>
        <td>Forces shape-based learning, reducing over-reliance on color cues</td>
      </tr>
    </tbody>
  </table>

  <h3>Advanced Domain Augmentation</h3>
  <p>
    Beyond standard augmentations, we employ domain-specific techniques to bridge the gap between the training desert and unseen test environments.
  </p>
  <ul>
    <li><strong>Fourier Domain Adaptation (FDA):</strong> Swaps low-frequency spectrum components between images, simulating different lighting and atmospheric conditions without altering semantic content.</li>
    <li><strong>Copy-Paste Augmentation:</strong> Rare class instances (flowers, logs) are extracted and pasted into diverse backgrounds, combating severe class imbalance in the dataset.</li>
    <li><strong>CutMix / ClassMix:</strong> Merges regions from different training images to create novel scene compositions, improving the model's ability to handle unexpected class adjacencies.</li>
  </ul>

  <div class="figure">
    <img src="./report_assets/data_flow.svg" alt="Data Augmentation Flow">
    <div class="caption">Figure 2: Data augmentation pipeline showing the transformation flow from raw images through geometric, photometric, and domain-specific augmentations before model training.</div>
  </div>

  <!-- ============================================================ -->
  <!-- 5. LOSS FUNCTION DESIGN -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="loss-function">5. Loss Function Design</h2>

  <p>
    We use a multi-component loss function specifically designed to address the three main challenges of desert segmentation: class imbalance, small object detection, and sharp boundary delineation. The combined loss balances global metrics optimization with targeted handling of difficult pixels and rare classes.
  </p>

  <div class="highlight-box green">
    <strong>Combined Loss:</strong> L = 0.5 &times; L<sub>Focal</sub> + 0.3 &times; L<sub>Dice</sub> + 0.2 &times; L<sub>Boundary</sub>
  </div>

  <h3>Component 1: Focal Loss (weight: 0.5)</h3>
  <p>
    Focal Loss addresses the severe class imbalance in the desert dataset, where Sky pixels dominate while Flowers and Logs are extremely rare. By down-weighting well-classified (easy) pixels and focusing gradient updates on hard, misclassified pixels, the model learns to discriminate subtle differences between visually similar classes.
  </p>
  <div class="formula-block">
    L<sub>Focal</sub>(p<sub>t</sub>) = -&alpha;<sub>t</sub> (1 - p<sub>t</sub>)<sup>&gamma;</sup> log(p<sub>t</sub>)<br><br>
    where &gamma; = 2.0 (focusing parameter), &alpha; = class-dependent weight<br>
    p<sub>t</sub> = predicted probability for the true class
  </div>
  <p>
    The focusing parameter &gamma;=2.0 ensures that pixels with prediction confidence above ~0.8 contribute negligibly to the loss, directing optimization effort toward ambiguous boundaries and rare classes.
  </p>

  <h3>Component 2: Dice Loss (weight: 0.3)</h3>
  <p>
    Dice Loss directly optimizes the overlap between predicted and ground-truth segmentation masks, making it a natural proxy for the IoU evaluation metric. Unlike cross-entropy, Dice Loss is less sensitive to class frequency, providing balanced gradients across all 10 classes regardless of pixel count.
  </p>
  <div class="formula-block">
    L<sub>Dice</sub> = 1 - (2 &times; |P &cap; G| + &epsilon;) / (|P| + |G| + &epsilon;)<br><br>
    where P = predicted mask, G = ground truth mask<br>
    &epsilon; = smoothing factor (1e-6) for numerical stability
  </div>

  <h3>Component 3: Boundary / Weighted Cross-Entropy Loss (weight: 0.2)</h3>
  <p>
    The boundary component applies class-specific weights to penalize misclassification of rare classes more heavily. This explicitly upweights classes like Flowers, Logs, and Ground Clutter that occupy less than 3% of total pixels, ensuring the model does not simply ignore them in favor of dominant classes.
  </p>
  <div class="formula-block">
    L<sub>WCE</sub> = -&sum;<sub>c</sub> w<sub>c</sub> &times; y<sub>c</sub> &times; log(p&#770;<sub>c</sub>)<br><br>
    where w<sub>c</sub> = class weight (inverse frequency),<br>
    Sky: low weight, Flowers/Logs: high weight
  </div>

  <h3>Why This Combination Works</h3>
  <ul>
    <li><strong>Focal Loss</strong> handles the per-pixel difficulty distribution, focusing on hard boundaries and ambiguous regions.</li>
    <li><strong>Dice Loss</strong> ensures the model optimizes for the actual evaluation metric (IoU), providing global region-level feedback.</li>
    <li><strong>Weighted CE</strong> provides explicit class-level rebalancing, guaranteeing that rare classes receive meaningful gradient signals during training.</li>
  </ul>

  <!-- ============================================================ -->
  <!-- 6. TRAINING CONFIGURATION -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="training-config">6. Training Configuration</h2>

  <p>
    Training was conducted in two phases on a Google Colab T4 GPU with mixed precision (fp16) enabled for memory efficiency and speed. The configuration was optimized through systematic experimentation on the validation set.
  </p>

  <table>
    <thead>
      <tr>
        <th>Parameter</th>
        <th>Value</th>
        <th>Notes</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Optimizer</strong></td>
        <td>AdamW</td>
        <td>Decoupled weight decay for better generalization</td>
      </tr>
      <tr>
        <td><strong>Backbone Learning Rate</strong></td>
        <td>6e-5</td>
        <td>Lower LR preserves pretrained ImageNet features</td>
      </tr>
      <tr>
        <td><strong>Decoder Learning Rate</strong></td>
        <td>6e-4</td>
        <td>10x higher -- decoder adapts faster to desert domain</td>
      </tr>
      <tr>
        <td><strong>Weight Decay</strong></td>
        <td>0.01</td>
        <td>L2 regularization to prevent overfitting</td>
      </tr>
      <tr>
        <td><strong>LR Schedule</strong></td>
        <td>Polynomial decay with warmup</td>
        <td>5% warmup, power=0.9 decay</td>
      </tr>
      <tr>
        <td><strong>Total Epochs</strong></td>
        <td>33 (13 + 20)</td>
        <td>Phase 1: initial training; Phase 2: fine-tuning</td>
      </tr>
      <tr>
        <td><strong>Batch Size</strong></td>
        <td>4 (effective 8)</td>
        <td>Gradient accumulation over 2 steps</td>
      </tr>
      <tr>
        <td><strong>Image Size</strong></td>
        <td>512 x 512</td>
        <td>Resized from 960x540 with random crops</td>
      </tr>
      <tr>
        <td><strong>Mixed Precision</strong></td>
        <td>fp16</td>
        <td>2x speedup, 50% VRAM reduction</td>
      </tr>
      <tr>
        <td><strong>Platform</strong></td>
        <td>Google Colab T4 GPU</td>
        <td>~5.5 hours total training time</td>
      </tr>
    </tbody>
  </table>

  <h3>Phase-Based Training Strategy</h3>

  <div class="highlight-box">
    <strong>Phase 1 (Epochs 1-13):</strong> Initial training on the full dataset with standard augmentations. The model converges rapidly, achieving peak validation mIoU of <strong>64.81%</strong> at epoch 13. This checkpoint is saved as the best model.
  </div>

  <div class="highlight-box">
    <strong>Phase 2 (Epochs 14-33):</strong> Fine-tuning with enhanced augmentations and domain adaptation techniques (FDA, Copy-Paste). This phase focuses on improving robustness to domain shift, with more aggressive regularization to prevent overfitting.
  </div>

  <h3>Differential Learning Rates</h3>
  <p>
    The 10x difference between backbone (6e-5) and decoder (6e-4) learning rates is a critical design choice. The pretrained encoder already contains powerful visual features from ImageNet; applying a small learning rate preserves these while allowing gentle adaptation to desert-specific patterns. The randomly initialized decoder, in contrast, needs to learn the class-specific feature fusion from scratch and benefits from faster updates.
  </p>

  <div class="figure">
    <img src="./report_assets/training_pipeline.svg" alt="Training Pipeline">
    <div class="caption">Figure 3: End-to-end training pipeline showing data loading, augmentation, model forward pass, multi-component loss computation, and optimizer step with gradient accumulation.</div>
  </div>

  <!-- ============================================================ -->
  <!-- 7. TRAINING RESULTS & ANALYSIS -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="training-results">7. Training Results &amp; Analysis</h2>

  <p>
    This section presents a comprehensive analysis of training performance, validation metrics, and per-class segmentation quality. The model demonstrates strong learning dynamics with rapid convergence and significant improvement over the baseline.
  </p>

  <!-- Key Result Callout -->
  <div class="metric-cards">
    <div class="metric-card">
      <span class="metric-value">64.81%</span>
      <span class="metric-label">Best mIoU (Epoch 13)</span>
    </div>
    <div class="metric-card">
      <span class="metric-value">2.6x</span>
      <span class="metric-label">vs Baseline (24.78%)</span>
    </div>
    <div class="metric-card">
      <span class="metric-value">0.7736</span>
      <span class="metric-label">Best mDice Score</span>
    </div>
    <div class="metric-card">
      <span class="metric-value">87.2%</span>
      <span class="metric-label">Pixel Accuracy</span>
    </div>
  </div>

  <div class="highlight-box green">
    <strong>Headline Result:</strong> Our SegFormer-B4 model achieves a best mean IoU of 64.81% at epoch 13, representing a <strong>+40.03 percentage point absolute improvement</strong> (2.6x) over the DINOv2 frozen-encoder baseline of 24.78%. This validates the importance of end-to-end fine-tuning and multi-scale feature extraction for domain-transfer desert segmentation.
  </div>

  <!-- Training Curves -->
  <h3>Training Curves Overview</h3>
  <div class="figure">
    <img src="./test_predictions/training_curves.png" alt="Training Curves Dashboard">
    <div class="caption">Figure 4: Training dashboard showing (top-left) training loss, (top-right) validation loss, (bottom-left) mIoU progression, and (bottom-right) learning rate schedule across 33 epochs.</div>
  </div>
  <p>
    The training loss decreases rapidly in the first 5 epochs, indicating effective initialization from ImageNet-pretrained weights. The mIoU curve climbs steeply during Phase 1, peaking at epoch 13 (64.81%), then fluctuates during Phase 2 fine-tuning as the model adapts to more aggressive augmentations. The polynomial learning rate schedule with warmup provides smooth convergence without sudden instabilities.
  </p>

  <!-- Train vs Val Loss -->
  <h3>Overfitting Analysis</h3>
  <div class="figure">
    <img src="./test_predictions/train_vs_val_loss.png" alt="Training vs Validation Loss">
    <div class="caption">Figure 5: Training loss vs. validation loss comparison across all epochs, showing convergence behavior and overfitting analysis.</div>
  </div>
  <p>
    The training and validation loss curves track closely throughout Phase 1, indicating healthy generalization without overfitting. The gap between curves remains minimal (&lt;0.03), confirming that our augmentation strategy and weight decay effectively regularize the model. The near-convergence of final training loss (0.185) and validation loss (0.186) at the end of training further validates this.
  </p>

  <!-- mDice and Accuracy -->
  <h3>Dice Coefficient &amp; Pixel Accuracy</h3>
  <div class="figure">
    <img src="./test_predictions/mdice_accuracy.png" alt="mDice and Pixel Accuracy Curves">
    <div class="caption">Figure 6: Mean Dice coefficient and pixel-level accuracy progression across training epochs.</div>
  </div>
  <p>
    The mean Dice coefficient reaches 0.7736 at the best epoch, closely correlating with the IoU improvement. Pixel-level accuracy achieves 87.2%, though this metric is dominated by frequent classes (Sky, Landscape). The Dice score provides a more balanced view of per-class performance, confirming strong overlap between predictions and ground truth across all classes.
  </p>

  <!-- Per-Class IoU Epochs -->
  <h3>Per-Class IoU Progression</h3>
  <div class="figure">
    <img src="./test_predictions/per_class_iou_epochs.png" alt="Per-Class IoU Over Epochs">
    <div class="caption">Figure 7: Individual class IoU progression across training epochs, showing convergence patterns and learning dynamics for each semantic class.</div>
  </div>
  <p>
    Sky converges earliest and highest (98.5% IoU), benefiting from its distinctive visual appearance and dominant pixel count. Landscape and Trees reach stable performance by epoch 8-10. The most challenging classes -- Ground Clutter (38.0%), Dry Bushes (49.0%), and Rocks (49.5%) -- continue improving throughout training but plateau at lower IoU values due to visual ambiguity and class overlap.
  </p>

  <!-- Final Per-Class IoU Bar -->
  <h3>Final Per-Class IoU Breakdown</h3>
  <div class="figure">
    <img src="./test_predictions/final_per_class_iou.png" alt="Final Per-Class IoU Bar Chart">
    <div class="caption">Figure 8: Per-class IoU at the best epoch (epoch 13), showing performance distribution across all 10 semantic classes.</div>
  </div>

  <table>
    <thead>
      <tr>
        <th>Class</th>
        <th>IoU</th>
        <th>Performance Tier</th>
        <th>Analysis</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><span class="color-swatch" style="background:#87CEEB;"></span>Sky</td>
        <td><strong style="color:#059669;">0.985</strong></td>
        <td>Excellent</td>
        <td>Visually distinct, dominant class with clear boundaries</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#228B22;"></span>Trees</td>
        <td><strong style="color:#059669;">0.856</strong></td>
        <td>Excellent</td>
        <td>Strong texture and color features, well-separated from background</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#00FF7F;"></span>Lush Bushes</td>
        <td><strong style="color:#059669;">0.695</strong></td>
        <td>Good</td>
        <td>Green vegetation is distinctive; some confusion with Trees at boundaries</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#BDB76B;"></span>Dry Grass</td>
        <td><strong style="color:#059669;">0.695</strong></td>
        <td>Good</td>
        <td>Reasonable performance despite similarity to Dry Bushes and Landscape</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#D2B48C;"></span>Landscape</td>
        <td><strong style="color:#059669;">0.682</strong></td>
        <td>Good</td>
        <td>Large homogeneous regions; boundary confusion with Dry Grass</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#FF69B4;"></span>Flowers</td>
        <td><strong>0.653</strong></td>
        <td>Moderate</td>
        <td>Rare class; copy-paste augmentation significantly boosted performance</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#8B4513;"></span>Logs</td>
        <td><strong>0.549</strong></td>
        <td>Moderate</td>
        <td>Small, rare objects; multi-scale features help but size remains a challenge</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#808080;"></span>Rocks</td>
        <td><strong>0.495</strong></td>
        <td>Moderate</td>
        <td>Confusion with Ground Clutter and Landscape; varied appearance</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#8B7765;"></span>Dry Bushes</td>
        <td><strong style="color:#DC2626;">0.490</strong></td>
        <td>Challenging</td>
        <td>Visually similar to Dry Grass and Landscape; brown-on-brown boundaries</td>
      </tr>
      <tr>
        <td><span class="color-swatch" style="background:#A0522D;"></span>Ground Clutter</td>
        <td><strong style="color:#DC2626;">0.380</strong></td>
        <td>Challenging</td>
        <td>Ill-defined catch-all class; inherent labeling ambiguity in dataset</td>
      </tr>
    </tbody>
  </table>

  <!-- All Metrics Overview -->
  <h3>Comprehensive Metrics Dashboard</h3>
  <div class="figure">
    <img src="./test_predictions/all_metrics_overview.png" alt="All Metrics Overview Dashboard">
    <div class="caption">Figure 9: Comprehensive 3x2 metrics dashboard combining all training metrics, loss curves, IoU progression, and class-level analysis in a single view.</div>
  </div>
  <p>
    The comprehensive dashboard confirms consistent improvement across all metrics simultaneously. No single metric improves at the expense of others, indicating a well-balanced training process. The final model demonstrates strong overall performance while maintaining acceptable accuracy even on the most challenging classes.
  </p>

  <!-- ============================================================ -->
  <!-- 8. TEST-TIME AUGMENTATION & INFERENCE -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="tta-inference">8. Test-Time Augmentation &amp; Inference</h2>

  <p>
    Test-Time Augmentation (TTA) is a critical inference-time technique that improves prediction quality by averaging predictions across multiple augmented views of each input image. This reduces sensitivity to spatial orientation and scale, yielding more robust and smoother segmentation outputs.
  </p>

  <h3>TTA Strategy</h3>

  <div class="metric-cards">
    <div class="metric-card blue">
      <span class="metric-value">3</span>
      <span class="metric-label">Scale Factors</span>
    </div>
    <div class="metric-card blue">
      <span class="metric-value">2</span>
      <span class="metric-label">Flip Variants</span>
    </div>
    <div class="metric-card blue">
      <span class="metric-value">6</span>
      <span class="metric-label">Total Forward Passes</span>
    </div>
    <div class="metric-card">
      <span class="metric-value">+2-4%</span>
      <span class="metric-label">Expected mIoU Boost</span>
    </div>
  </div>

  <table>
    <thead>
      <tr>
        <th>Augmentation</th>
        <th>Values</th>
        <th>Purpose</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Multi-Scale</strong></td>
        <td>0.75x, 1.0x, 1.25x</td>
        <td>Captures objects at different resolutions; small objects benefit from upscaling</td>
      </tr>
      <tr>
        <td><strong>Horizontal Flip</strong></td>
        <td>Original + Flipped</td>
        <td>Reduces directional bias in predictions</td>
      </tr>
    </tbody>
  </table>

  <h3>TTA Process</h3>
  <ol>
    <li>For each of the 3 scales, resize the input image accordingly.</li>
    <li>For each scaled image, generate both the original and horizontally flipped version (2 variants).</li>
    <li>Normalize all 6 images with ImageNet statistics (mean, std).</li>
    <li>Run each through the model, obtaining softmax probability maps.</li>
    <li>Reverse spatial transformations (un-flip, resize back to original dimensions).</li>
    <li>Average all 6 softmax probability maps element-wise.</li>
    <li>Apply argmax to the averaged probabilities for the final pixel-wise prediction.</li>
  </ol>

  <div class="figure">
    <img src="./report_assets/tta_inference.svg" alt="TTA Inference Pipeline">
    <div class="caption">Figure 10: Test-Time Augmentation pipeline showing multi-scale and flip augmentations, parallel inference, and softmax probability averaging for final prediction.</div>
  </div>

  <h3>Inference Speed</h3>
  <table>
    <thead>
      <tr>
        <th>Mode</th>
        <th>M4 Pro (MPS)</th>
        <th>T4 GPU (CUDA)</th>
        <th>Use Case</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Single-scale</strong></td>
        <td>~30-40 ms (25-33 FPS)</td>
        <td>~50 ms (20 FPS)</td>
        <td>Real-time UGV navigation</td>
      </tr>
      <tr>
        <td><strong>TTA (6-pass)</strong></td>
        <td>~200-300 ms (3-5 FPS)</td>
        <td>~300-400 ms</td>
        <td>Offline evaluation, high-accuracy mode</td>
      </tr>
    </tbody>
  </table>

  <!-- ============================================================ -->
  <!-- 9. TEST SET EVALUATION -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="test-evaluation">9. Test Set Evaluation</h2>

  <p>
    The test set consists of images from a different digital twin desert environment, serving as the primary evaluation of domain generalization. This section analyzes how well the model transfers learned features from the training desert to the unseen test domain.
  </p>

  <h3>Domain Shift Analysis</h3>
  <p>
    The test desert differs from the training environment in terrain layout, vegetation density, lighting conditions, and scene composition. Despite these differences, the model demonstrates strong transfer for structurally distinct classes (Sky, Trees) while showing expected degradation on visually ambiguous classes (Ground Clutter, Dry Bushes) where domain-specific texture patterns are less transferable.
  </p>

  <div class="highlight-box">
    <strong>Key Observation:</strong> Classes with strong semantic priors (Sky is always blue and above the horizon; Trees have distinctive green canopy structures) transfer well across domains. Classes defined primarily by texture (Ground Clutter, Dry Bushes) suffer the most from domain shift, as texture patterns are inherently domain-specific.
  </div>

  <h3>Class Transfer Analysis</h3>
  <table>
    <thead>
      <tr>
        <th>Transfer Quality</th>
        <th>Classes</th>
        <th>Reason</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong style="color:#059669;">Strong Transfer</strong></td>
        <td>Sky, Trees, Lush Bushes</td>
        <td>Distinctive color and shape features that are domain-invariant</td>
      </tr>
      <tr>
        <td><strong style="color:#D97706;">Moderate Transfer</strong></td>
        <td>Landscape, Dry Grass, Flowers, Rocks</td>
        <td>Partially domain-dependent; augmentation helps bridge the gap</td>
      </tr>
      <tr>
        <td><strong style="color:#DC2626;">Challenging Transfer</strong></td>
        <td>Dry Bushes, Ground Clutter, Logs</td>
        <td>Texture-dependent classes with high domain specificity</td>
      </tr>
    </tbody>
  </table>

  <h3>Qualitative Results</h3>
  <p>
    The model produces visually coherent segmentation maps on the test set, correctly identifying large-scale structures (sky boundaries, tree canopies, ground plane) while occasionally struggling with fine boundaries between visually similar brown-toned classes. Fourier Domain Adaptation and heavy color jitter during training measurably improve test-domain performance.
  </p>

  <!-- ============================================================ -->
  <!-- 10. REAL-TIME DEMO ARCHITECTURE -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="demo-architecture">10. Real-Time Demo Architecture</h2>

  <p>
    We developed an interactive real-time demonstration showcasing the segmentation model in a simulated UGV navigation scenario. The demo combines a 3D desert environment with live model inference to visualize how the system would operate during autonomous navigation.
  </p>

  <h3>System Architecture</h3>

  <div class="arch-diagram">
    <div class="arch-box">Three.js Frontend<small>3D Desert Scene</small></div>
    <span class="arch-arrow">&#8594;</span>
    <div class="arch-box purple">Flask API<small>REST Endpoint</small></div>
    <span class="arch-arrow">&#8594;</span>
    <div class="arch-box green">SegFormer-B4<small>PyTorch MPS</small></div>
    <span class="arch-arrow">&#8594;</span>
    <div class="arch-box gray">Segmentation Overlay<small>Color-coded Mask</small></div>
  </div>

  <h3>Component Details</h3>

  <table>
    <thead>
      <tr>
        <th>Component</th>
        <th>Technology</th>
        <th>Role</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>3D Scene</strong></td>
        <td>Three.js + React</td>
        <td>Procedural desert terrain with sand, vegetation, rocks, and sky</td>
      </tr>
      <tr>
        <td><strong>Rover Model</strong></td>
        <td>Three.js GLTF/OBJ</td>
        <td>UGV model navigating predefined waypoints through the desert</td>
      </tr>
      <tr>
        <td><strong>Camera Feed</strong></td>
        <td>Three.js WebGLRenderer</td>
        <td>Renders rover's first-person view, captures frames as images</td>
      </tr>
      <tr>
        <td><strong>Backend API</strong></td>
        <td>Flask + PyTorch</td>
        <td>Receives camera frames, runs SegFormer inference, returns masks</td>
      </tr>
      <tr>
        <td><strong>Segmentation Overlay</strong></td>
        <td>HTML Canvas</td>
        <td>Overlays color-coded segmentation mask on the camera feed</td>
      </tr>
      <tr>
        <td><strong>Dashboard</strong></td>
        <td>React Components</td>
        <td>Class distribution bar, confidence scores, safe zone indicators</td>
      </tr>
    </tbody>
  </table>

  <h3>Data Flow</h3>
  <ol>
    <li>The Three.js scene renders the rover's camera perspective at 20+ FPS.</li>
    <li>Captured frames are sent to the Flask backend via WebSocket or REST POST.</li>
    <li>The backend preprocesses the image (resize to 512x512, normalize) and runs SegFormer-B4 inference on the MPS device.</li>
    <li>The predicted segmentation mask is color-coded and returned to the frontend.</li>
    <li>The overlay is rendered on the camera feed panel, and the dashboard updates class-level statistics in real time.</li>
  </ol>

  <h3>Performance</h3>
  <div class="metric-cards">
    <div class="metric-card">
      <span class="metric-value">20+ FPS</span>
      <span class="metric-label">3D Scene Rendering</span>
    </div>
    <div class="metric-card">
      <span class="metric-value">30-40 ms</span>
      <span class="metric-label">Inference Latency</span>
    </div>
    <div class="metric-card blue">
      <span class="metric-value">M4 Pro</span>
      <span class="metric-label">Target Hardware</span>
    </div>
  </div>

  <!-- ============================================================ -->
  <!-- 11. CHALLENGES & KEY LEARNINGS -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="challenges">11. Challenges &amp; Key Learnings</h2>

  <h3>Challenge 1: Domain Shift Between Synthetic Deserts</h3>
  <p>
    The fundamental challenge of this project is the domain gap between the training and test desert environments. Even though both are synthetic, they differ in terrain topology, vegetation placement, lighting angles, and sand textures. Models that achieve high validation IoU can still underperform on the test set if they learn domain-specific shortcuts rather than generalizable features.
  </p>
  <p>
    <strong>Mitigation:</strong> Fourier Domain Adaptation, aggressive ColorJitter, and multi-scale training force the model to rely on shape and structural cues rather than domain-specific color and texture patterns.
  </p>

  <h3>Challenge 2: Severe Class Imbalance</h3>
  <p>
    Sky pixels can constitute 30-50% of an image, while Flowers and Logs each occupy less than 2-3%. A naive cross-entropy loss would encourage the model to predict Sky everywhere, achieving high pixel accuracy but poor per-class IoU. This imbalance also manifests during gradient computation, where rare-class gradients are overwhelmed by dominant-class updates.
  </p>
  <p>
    <strong>Mitigation:</strong> The combined Focal + Dice + Weighted CE loss explicitly addresses this at three levels: per-pixel difficulty (Focal), per-class overlap (Dice), and frequency rebalancing (Weighted CE). Copy-paste augmentation also synthetically increases rare class representation.
  </p>

  <h3>Challenge 3: Visually Similar Classes</h3>
  <p>
    Dry Grass, Dry Bushes, Ground Clutter, and Landscape all share similar brown/tan color profiles. At class boundaries, even human annotators may struggle to distinguish them. This inter-class similarity is the primary bottleneck for mIoU improvement, as the model must learn subtle texture and context differences to discriminate between them.
  </p>
  <p>
    <strong>Mitigation:</strong> Multi-scale hierarchical features in SegFormer allow the model to use both local texture (fine stages) and global context (coarse stages) for disambiguation. CLAHE augmentation enhances local contrast to accentuate subtle differences.
  </p>

  <h3>Challenge 4: Training Stability with Combined Loss</h3>
  <p>
    The multi-component loss function introduces additional hyperparameters (loss weights, focal gamma, class weights) that interact in complex ways. Poorly balanced weights can cause training instability, with one loss component dominating and suppressing others. Early experiments showed oscillation in per-class IoU when the Dice loss weight was too high.
  </p>
  <p>
    <strong>Mitigation:</strong> Systematic ablation studies determined the optimal weight balance (0.5/0.3/0.2). The polynomial learning rate schedule with warmup prevents early training instability when the combined loss landscape is highly non-convex.
  </p>

  <h3>Key Learnings</h3>
  <ul>
    <li><strong>End-to-end fine-tuning vastly outperforms frozen encoders:</strong> The 40+ percentage point improvement over the DINOv2 baseline demonstrates that domain-specific adaptation of the encoder is essential for specialized segmentation tasks.</li>
    <li><strong>Augmentation is as important as architecture:</strong> Without our augmentation pipeline, the same SegFormer-B4 model achieves 10-15% lower mIoU, highlighting the critical role of data diversity for domain generalization.</li>
    <li><strong>Best model is not necessarily the last model:</strong> The best validation mIoU occurred at epoch 13 out of 33 total epochs. Saving checkpoints at peak performance and using early-stopping criteria is essential.</li>
    <li><strong>Per-class analysis reveals actionable insights:</strong> Global mIoU masks significant performance disparities. The 60+ point spread between Sky (98.5%) and Ground Clutter (38.0%) suggests that class-specific strategies (targeted augmentation, separate loss weights) can yield further improvements.</li>
  </ul>

  <!-- ============================================================ -->
  <!-- 12. CONCLUSION & FUTURE WORK -->
  <!-- ============================================================ -->
  <h2 class="section-title" id="conclusion">12. Conclusion &amp; Future Work</h2>

  <h3>Summary</h3>

  <div class="highlight-box green">
    <strong>Final Result:</strong> Our SegFormer-B4 model achieves <strong>64.81% mIoU</strong> on desert scene segmentation, a <strong>2.6x improvement</strong> over the 24.78% DINOv2 baseline. The model runs in real time at 25-33 FPS on Apple M4 Pro hardware and integrates into a live 3D navigation demo with Three.js and Flask.
  </div>

  <p>
    This project demonstrates that a well-designed segmentation pipeline -- combining a hierarchical transformer encoder with multi-component loss functions, aggressive domain augmentation, and test-time augmentation -- can achieve strong performance on the challenging task of cross-domain desert scene understanding. The end-to-end approach, from data preprocessing through real-time inference, shows practical viability for UGV navigation systems.
  </p>

  <div class="metric-cards">
    <div class="metric-card">
      <span class="metric-value">64.81%</span>
      <span class="metric-label">Best mIoU</span>
    </div>
    <div class="metric-card">
      <span class="metric-value">+40.03 pp</span>
      <span class="metric-label">Over Baseline</span>
    </div>
    <div class="metric-card">
      <span class="metric-value">87.2%</span>
      <span class="metric-label">Pixel Accuracy</span>
    </div>
    <div class="metric-card blue">
      <span class="metric-value">25-33 FPS</span>
      <span class="metric-label">Real-Time Capable</span>
    </div>
  </div>

  <h3>Future Work</h3>

  <table>
    <thead>
      <tr>
        <th>Direction</th>
        <th>Expected Impact</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Ensemble Methods</strong></td>
        <td>+3-5% mIoU</td>
        <td>Combine SegFormer-B4 with SegFormer-B2 and DeepLabV3+ predictions via weighted averaging; diverse architectures capture complementary features.</td>
      </tr>
      <tr>
        <td><strong>Larger Backbones</strong></td>
        <td>+2-4% mIoU</td>
        <td>SegFormer-B5 or Swin-Large backbones offer higher capacity; requires more GPU memory but can improve fine-grained class discrimination.</td>
      </tr>
      <tr>
        <td><strong>CRF Post-Processing</strong></td>
        <td>+1-2% mIoU</td>
        <td>Conditional Random Field refinement sharpens class boundaries using pixel color similarity, particularly benefiting brown-tone class boundaries.</td>
      </tr>
      <tr>
        <td><strong>Domain Augmentation</strong></td>
        <td>+2-5% mIoU</td>
        <td>More aggressive style transfer (AdaIN, neural style transfer) between desert domains; progressive domain randomization during training.</td>
      </tr>
      <tr>
        <td><strong>Uncertainty Quantification</strong></td>
        <td>Safety improvement</td>
        <td>MC Dropout or deep ensembles for pixel-level confidence estimation; critical for safety-aware UGV navigation decisions.</td>
      </tr>
      <tr>
        <td><strong>Edge Deployment</strong></td>
        <td>Practical deployment</td>
        <td>ONNX export and TensorRT optimization for NVIDIA Jetson; INT8 quantization for embedded deployment on actual UGV hardware.</td>
      </tr>
    </tbody>
  </table>

  <hr>

  <p style="text-align: center; color: #9CA3AF; font-size: 0.85rem; margin-top: 40px;">
    Semantic Segmentation for Desert UGV Navigation &mdash; Hackathon Submission<br>
    SegFormer-B4 | PyTorch | Albumentations | Three.js
  </p>

</div>
</body>
</html>

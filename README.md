# DuneNet

**Autonomous UGV Perception Platform for Off-Road Navigation**

DuneNet is a production-ready semantic segmentation system designed for autonomous unmanned ground vehicles (UGVs) operating in desert and off-road environments. The platform combines state-of-the-art deep learning with real-time 3D simulation to enable robust terrain traversability analysis and path planning.

## Live Deployment

- **Frontend**: [https://dunenet.vercel.app](https://dunenet.vercel.app)
- **API**: [https://parasssssssssssss-dunenet-api.hf.space](https://parasssssssssssss-dunenet-api.hf.space)
- **API Documentation**: [https://parasssssssssssss-dunenet-api.hf.space/docs](https://parasssssssssssss-dunenet-api.hf.space/docs)

## Overview

DuneNet addresses the critical challenge of terrain perception for autonomous navigation in unstructured outdoor environments. The system provides pixel-level semantic segmentation to classify terrain types and generate traversability cost maps for path planning algorithms.

### Key Capabilities

- **Real-time Semantic Segmentation**: Pixel-accurate classification of desert terrain elements
- **Traversability Analysis**: Automated generation of navigation cost maps
- **3D Digital Twin Simulation**: Interactive visualization of rover navigation with live inference
- **Production API**: RESTful endpoints for integration with autonomous systems
- **Visual Workflow System**: Interactive pipeline for algorithm development and debugging

## Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Client Applications                      â”‚
â”‚  (Web Dashboard, Mobile Apps, ROS Nodes, External Systems)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ HTTPS/REST
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Endpoints: /predict, /predict/sim, /model/info     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Segformer Model (nvidia/mit-b4)                    â”‚   â”‚
â”‚  â”‚  - 10 terrain classes                               â”‚   â”‚
â”‚  â”‚  - 512x512 input resolution                         â”‚   â”‚
â”‚  â”‚  - Traversability mapping engine                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Next.js Frontend                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Simulation: 3D rover navigation with live inferenceâ”‚   â”‚
â”‚  â”‚  Dashboard: Analytics and model performance metrics â”‚   â”‚
â”‚  â”‚  Assistant: AI-powered model analysis               â”‚   â”‚
â”‚  â”‚  Workflow: Visual pipeline editor                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**Backend**
- FastAPI 0.109.0
- PyTorch 2.6.0
- Transformers 4.30.0 (Hugging Face)
- Albumentations 1.3.0
- Docker containerization

**Frontend**
- Next.js 16.1.6
- React 19.2.0
- Three.js + React Three Fiber
- Tailwind CSS 4
- Framer Motion

**Machine Learning**
- Model: Segformer (nvidia/mit-b4)
- Framework: PyTorch
- Training: Custom desert terrain dataset
- Deployment: CPU-optimized inference

## Model Specifications

### Segformer Architecture

```
Model: nvidia/mit-b4 (Hierarchical Transformer Encoder)
Input Size: 512 Ã— 512 Ã— 3
Output: 512 Ã— 512 Ã— 10 (per-pixel class probabilities)
Parameters: ~64M
Encoder Depths: [3, 8, 27, 3]
Embedding Dimensions: [64, 128, 320, 512]
```

### Training Performance

| Metric | Best (Epoch 13) | Final (Epoch 33) |
|--------|-----------------|------------------|
| Validation mIoU | **64.81%** | 57.92% |
| Mean Dice | 72.76% | 68.34% |
| Pixel Accuracy | 87.28% | 85.12% |
| Training Loss | 0.198 | 0.165 |
| Validation Loss | 0.175 | 0.188 |

### Semantic Classes

| ID | Class | Description | Traversability |
|----|-------|-------------|----------------|
| 0 | Trees | Large vegetation obstacles | No-Go |
| 1 | Lush Bushes | Dense shrubs | No-Go |
| 2 | Dry Grass | Short sparse vegetation | Go |
| 3 | Dry Bushes | Sparse shrubs | Caution |
| 4 | Ground Clutter | Small debris and rocks | Caution |
| 5 | Flowers | Low vegetation | Go |
| 6 | Logs | Fallen tree trunks | No-Go |
| 7 | Rocks | Large stone obstacles | Caution |
| 8 | Landscape | Open terrain | Go |
| 9 | Sky | Background | Sky |

### Per-Class Performance (IoU)

```
Trees:          78.94%
Lush Bushes:    55.44%
Dry Grass:      63.77%
Dry Bushes:     29.15%
Ground Clutter: 24.70%
Flowers:        51.77%
Logs:           23.63%
Rocks:          32.26%
Landscape:      64.26%
Sky:            98.18%
```

## Installation

### Prerequisites

- Python 3.12+
- Node.js 20+
- CUDA 12.1+ (optional, for GPU acceleration)
- Git Large File Storage (for model weights)

### Backend Setup

```bash
# Clone repository
git clone https://github.com/SPIT-Hackathon-2026/pyTeam.git
cd pyTeam

# Create Python virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
cd api_server
pip install -r requirements.txt

# Verify model file exists
ls models/latest_model_ft.pth  # Should be ~245 MB

# Start server
python main.py
# Server runs on http://localhost:7860
```

### Frontend Setup

```bash
# Navigate to frontend directory
cd DuneNet

# Install dependencies
npm install

# Configure environment variables
cp .env.example .env.local
# Edit .env.local with your API URLs and credentials

# Start development server
npm run dev
# Application runs on http://localhost:3000
```

### Docker Deployment

```bash
# Build backend container
cd api_server
docker build -t dunenet-api .

# Run container
docker run -p 7860:7860 dunenet-api

# For GPU support
docker run --gpus all -p 7860:7860 dunenet-api
```

## API Reference

### Base URL

```
Production: https://parasssssssssssss-dunenet-api.hf.space
Local: http://localhost:7860
```

### Endpoints

#### Health Check
```http
GET /
```

Response:
```json
{
  "status": "running",
  "model_loaded": true,
  "device": "cpu"
}
```

#### Image Segmentation
```http
POST /predict
Content-Type: multipart/form-data

file: <image_file>
```

Response:
```json
{
  "prediction": 8,
  "class_name": "Landscape",
  "confidence": 0.956,
  "device_used": "cpu",
  "class_distribution": {
    "Trees": 0.023,
    "Landscape": 0.956,
    "Sky": 0.021
  },
  "segmentation_mask": "<base64_encoded_image>",
  "overlay_image": "<base64_encoded_image>",
  "traversability_map": "<base64_encoded_image>",
  "traversability_overlay": "<base64_encoded_image>",
  "traversability_stats": {
    "safe_percent": 78.5,
    "caution_percent": 12.3,
    "blocked_percent": 9.2
  }
}
```

#### Simulation Inference
```http
POST /predict/sim
Content-Type: multipart/form-data

file: <image_file>
```

Response: Returns cost map grid for A* path planning

#### Model Information
```http
GET /model/info
```

Response:
```json
{
  "model_name": "nvidia/mit-b4",
  "num_classes": 10,
  "input_size": 512,
  "device": "cpu",
  "classes": [...],
  "checkpoint_info": {
    "epoch": 20,
    "miou": 0.5792
  }
}
```

### Authentication

Current deployment uses public endpoints. For production use with authentication:

```http
Authorization: Bearer <your_api_token>
```

## Project Structure

```
DuneNet/
â”œâ”€â”€ api_server/                      # Backend API
â”‚   â”œâ”€â”€ main.py                      # FastAPI application
â”‚   â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile                   # Container definition
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ latest_model_ft.pth      # Trained model weights (245 MB)
â”‚   â””â”€â”€ test_api.py                  # API test suite
â”‚
â”œâ”€â”€ DuneNet/                         # Frontend application
â”‚   â”œâ”€â”€ app/                         # Next.js app router
â”‚   â”‚   â”œâ”€â”€ simulation/              # 3D rover simulation
â”‚   â”‚   â”œâ”€â”€ dashboard/               # Analytics dashboard
â”‚   â”‚   â”œâ”€â”€ assistant/               # AI model analysis
â”‚   â”‚   â””â”€â”€ api/                     # API routes
â”‚   â”œâ”€â”€ components/                  # React components
â”‚   â”‚   â”œâ”€â”€ simulation/              # 3D scene components
â”‚   â”‚   â”œâ”€â”€ ModelInference.jsx       # Inference interface
â”‚   â”‚   â””â”€â”€ SegmentationVisualizer.jsx
â”‚   â”œâ”€â”€ lib/                         # Utilities
â”‚   â””â”€â”€ public/                      # Static assets
â”‚       â”œâ”€â”€ models/                  # 3D models (GLB)
â”‚       â””â”€â”€ textures/                # Terrain textures
â”‚
â”œâ”€â”€ segformer_desert_segmentation.ipynb  # Training notebook
â”œâ”€â”€ training_metrics.json            # Full training history
â”œâ”€â”€ report.html                      # Training analysis report
â””â”€â”€ README.md                        # This file
```

## Usage Examples

### Python Client

```python
import requests

# Endpoint
url = "https://parasssssssssssss-dunenet-api.hf.space/predict"

# Upload image
with open("terrain_image.jpg", "rb") as f:
    response = requests.post(
        url,
        files={"file": f}
    )

result = response.json()
print(f"Predicted class: {result['class_name']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Safe terrain: {result['traversability_stats']['safe_percent']:.1f}%")
```

### JavaScript Client

```javascript
const formData = new FormData();
formData.append('file', imageFile);

const response = await fetch(
  'https://parasssssssssssss-dunenet-api.hf.space/predict',
  {
    method: 'POST',
    body: formData
  }
);

const result = await response.json();
console.log('Prediction:', result.class_name);
console.log('Traversability:', result.traversability_stats);
```

### cURL

```bash
curl -X POST \
  https://parasssssssssssss-dunenet-api.hf.space/predict \
  -F "file=@terrain_image.jpg" \
  -o response.json
```

## Performance Benchmarks

### Inference Latency

| Hardware | Resolution | Time (ms) | FPS |
|----------|-----------|-----------|-----|
| NVIDIA RTX 3090 | 512Ã—512 | 12 | 83 |
| NVIDIA T4 | 512Ã—512 | 28 | 36 |
| Intel Xeon (CPU) | 512Ã—512 | 340 | 3 |
| Apple M1 Pro | 512Ã—512 | 180 | 6 |

### Throughput

- Single image: 340ms (CPU) / 28ms (GPU)
- Batch inference (16 images): 2.1s (CPU) / 180ms (GPU)
- API overhead: ~15-30ms
- Total end-to-end: 400ms (CPU) / 60ms (GPU)

## Development

### Training Custom Models

```bash
# Open training notebook
jupyter notebook segformer_desert_segmentation.ipynb

# Or use the training script
python train.py \
  --data_dir ./dataset \
  --model nvidia/mit-b4 \
  --num_classes 10 \
  --epochs 30 \
  --batch_size 8 \
  --lr 2e-5
```

### Running Tests

```bash
# Backend tests
cd api_server
python test_api.py

# Frontend tests
cd DuneNet
npm test

# E2E tests
npm run test:e2e
```

### Code Quality

```bash
# Linting
npm run lint
pylint api_server/main.py

# Formatting
npm run format
black api_server/

# Type checking
npm run type-check
mypy api_server/
```

## Deployment

### Vercel (Frontend)

```bash
# Install Vercel CLI
npm i -g vercel

# Deploy
cd DuneNet
vercel --prod

# Configure environment variables in Vercel dashboard:
# - NEXT_PUBLIC_API_URL
# - NEXTAUTH_URL
# - NEXTAUTH_SECRET
# - MONGODB_URI
# - GOOGLE_CLIENT_ID
# - GOOGLE_CLIENT_SECRET
```

### Hugging Face Spaces (Backend)

```bash
# Install Git LFS
git lfs install

# Track model file
git lfs track "*.pth"

# Push to Hugging Face
git remote add hf https://huggingface.co/spaces/<username>/<space-name>
git push hf main
```

### Docker Compose

```yaml
version: '3.8'
services:
  api:
    build: ./api_server
    ports:
      - "7860:7860"
    environment:
      - PORT=7860
  
  frontend:
    build: ./DuneNet
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://api:7860
```

## Configuration

### Environment Variables

**Backend** (`.env` in `api_server/`)
```bash
PORT=7860
NUM_CLASSES=10
IMG_SIZE=512
MODEL_NAME=nvidia/mit-b4
```

**Frontend** (`.env.local` in `DuneNet/`)
```bash
NEXT_PUBLIC_API_URL=https://parasssssssssssss-dunenet-api.hf.space
NEXTAUTH_URL=https://dunenet.vercel.app
NEXTAUTH_SECRET=<your_secret>
MONGODB_URI=<your_mongodb_connection_string>
GOOGLE_CLIENT_ID=<your_google_oauth_client_id>
GOOGLE_CLIENT_SECRET=<your_google_oauth_secret>
GEMINI_API_KEY=<your_gemini_api_key>
ELEVENLABS_API_KEY=<your_elevenlabs_api_key>
```

## Troubleshooting

### Model Loading Issues

```bash
# Verify model file integrity
ls -lh api_server/models/latest_model_ft.pth
# Should be approximately 245 MB

# Check Git LFS
git lfs ls-files
```

### CORS Errors

Ensure the API server CORS configuration includes your frontend domain:
```python
allow_origins=[
    "http://localhost:3000",
    "https://dunenet.vercel.app",
    "https://*.vercel.app",
]
```

### Memory Issues

For memory-constrained environments:
```python
# Reduce batch size
BATCH_SIZE = 1

# Use mixed precision
torch.set_float32_matmul_precision('high')
```

### WebGL Context Loss

If 3D simulation shows black screen:
- Check browser WebGL support: `about:gpu`
- Disable hardware acceleration
- Reduce terrain segments in settings
- Enable GPU safe mode in simulation

## Contributing

We welcome contributions from the community. Please follow these guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/enhancement`)
3. Commit your changes (`git commit -m 'Add enhancement'`)
4. Push to branch (`git push origin feature/enhancement`)
5. Open a Pull Request

### Code Standards

- Python: PEP 8, type hints required
- JavaScript: ESLint configuration, Prettier formatting
- Commits: Conventional Commits specification
- Documentation: Update README for new features

## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.

## Acknowledgments

- **Model Architecture**: Segformer by Xie et al. (NVIDIA Research)
- **Framework**: Hugging Face Transformers
- **Frontend Libraries**: React Three Fiber, Drei, Postprocessing
- **Deployment**: Vercel, Hugging Face Spaces

## Citation

If you use DuneNet in your research, please cite:

```bibtex
@software{dunenet2026,
  title={DuneNet: Autonomous UGV Perception Platform},
  author={Team pyTeam},
  year={2026},
  url={https://github.com/SPIT-Hackathon-2026/pyTeam}
}
```

## Contact

For questions, issues, or collaborations:

- **GitHub Issues**: [https://github.com/SPIT-Hackathon-2026/pyTeam/issues](https://github.com/SPIT-Hackathon-2026/pyTeam/issues)
- **Project Website**: [https://dunenet.vercel.app](https://dunenet.vercel.app)

---

**Status**: Production Ready | **Version**: 1.0.0 | **Last Updated**: February 2026

1. Test with your images
2. Customize preprocessing
3. Add more features
4. Optimize performance
5. Deploy to production

## ğŸ“ Need Help?

1. Check the [documentation](INDEX.md)
2. Run the test script
3. Check console logs
4. Review error messages

## ğŸ‰ Success!

If you can:
- âœ“ Start both servers
- âœ“ Access the dashboard
- âœ“ Upload images
- âœ“ Get predictions

**You're all set! Happy inferencing! ğŸŠ**

---

**Documentation**: [INDEX.md](INDEX.md) | **Quick Start**: [QUICK_START.md](QUICK_START.md) | **API Docs**: [README_API.md](README_API.md)
